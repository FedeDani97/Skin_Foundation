{"cells":[{"cell_type":"markdown","source":["# **FROZEN BACKBONE (GDF / PanDERM/ DINOV2) + shared LR head**\n","\n"," - Safe PCA, class_weight=balanced\n"," - TTA = average features per image before LR\n"," - Binary threshold tuning on validation\n"," - Unified metrics + Repeat-N wrapper"],"metadata":{"id":"TI1dPjvJFD3d"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"fcc3Gh8C286r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================================================\n","# Frozen backbones (GDF / PanDerm / DINOv2) + shared LR head\n","# - Safe PCA, class_weight=balanced\n","# - TTA = average features per image before LR\n","# - Binary threshold tuning on validation\n","# - Unified metrics + Repeat-N wrapper\n","# ================================================================\n","\n","# --- Core imports\n","import os, io, math, gc, time, random, hashlib, warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","\n","# Torch / timm (for PanDerm/DINOv2 backbones)\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","import timm\n","from timm.data import resolve_data_config\n","\n","# TF (for GDF backbone embeddings)\n","import tensorflow as tf\n","from huggingface_hub import from_pretrained_keras\n","\n","# Sklearn\n","from sklearn.model_selection import StratifiedGroupKFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import (\n","    f1_score, accuracy_score, matthews_corrcoef, cohen_kappa_score,\n","    top_k_accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",")\n","from sklearn.dummy import DummyClassifier\n","\n","# Repro\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","GLOBAL_SEED = 42\n","random.seed(GLOBAL_SEED); np.random.seed(GLOBAL_SEED); torch.manual_seed(GLOBAL_SEED)\n","if torch.cuda.is_available(): torch.cuda.manual_seed_all(GLOBAL_SEED)"],"metadata":{"id":"KlEUPY4AFYHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iE4e5gb6Ld3E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------- GDF embedding cache ----------\n","CACHE_DIR = \"/content/drive/MyDrive/Skin_project/derm_emb_cache\"  # your existing cache\n","os.makedirs(CACHE_DIR, exist_ok=True)\n","\n","def _sha1(s: str) -> str:\n","    import hashlib\n","    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n","\n","def _emb_cache_path(img_path: str, variant: str = \"single\") -> str:\n","    # variant = \"single\" for one-view; \"tta10\" (or similar) for TTA-averaged cache\n","    base = _sha1(f\"{img_path}::{variant}\")\n","    return os.path.join(CACHE_DIR, base + \".npy\")\n","\n","def _load_embed_if_cached(img_path: str, variant: str = \"single\"):\n","    fp = _emb_cache_path(img_path, variant)\n","    if os.path.exists(fp):\n","        try:\n","            arr = np.load(fp, allow_pickle=False)\n","            if arr.ndim == 1:  # (6144,)\n","                return arr.astype(np.float32, copy=False)\n","        except Exception:\n","            pass\n","    return None\n","\n","def _save_embed_to_cache(img_path: str, emb: np.ndarray, variant: str = \"single\"):\n","    fp = _emb_cache_path(img_path, variant)\n","    try:\n","        np.save(fp, emb.astype(np.float32), allow_pickle=False)\n","    except Exception:\n","        pass\n"],"metadata":{"id":"InsAsplWLeZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aEO83YwjLeMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================================================\n","# Paths / Features\n","# ================================================================\n","CSV_TRAIN_POOL = \"/content/drive/MyDrive/Skin_project/train_original.csv\"\n","CSV_SYN_WIDE   = \"/content/drive/MyDrive/Skin_project/synthetic_as_wide.csv\"  # set True/False when running\n","CSV_TEST_FIXED = \"/content/drive/MyDrive/Skin_project/test_original.csv\"\n","\n","feature2imgcol = {\n","    \"moisture\": \"moisture_img\",\n","    \"oiliness\": \"oiliness_img\",\n","    \"elasticity\": \"elasticity_img\",\n","    \"texture\": \"texture_img\",\n","    \"hyperpigmentation\": \"hyperpigmentation_img\",\n","}\n","\n","DEFAULT_LABELS = [-1, 0, 1]\n","BINARY_MAP = {\n","    \"oiliness\": { -1: 0, 0: 1, 1: 1 },  # dry vs non-dry\n","    \"moisture\": { -1: 0, 0: 1, 1: 1 },\n","}\n","FEATURE_LABELS = {\n","    \"oiliness\":  [0, 1],\n","    \"moisture\":  [0, 1],\n","    \"texture\":   [-1, 0, 1],\n","    \"elasticity\":[-1, 0, 1],\n","    \"hyperpigmentation\":[-1, 0, 1],\n","}\n","\n","VAL_FRAC_WITHIN_TRAIN = 0.125"],"metadata":{"id":"lrAieZmAFcRL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================================================\n","# Dataframe helpers (shared with fine-tune notebook)\n","# ================================================================\n","def load_feature_df_one(csv_path, feature):\n","    df_all  = pd.read_csv(csv_path)\n","    img_col = feature2imgcol[feature]\n","    lbl_col = f\"{feature}_score\"\n","    cols = [\"patient_id\", img_col, lbl_col] + ([\"region\"] if \"region\" in df_all.columns else [])\n","    df  = df_all[cols].copy().rename(columns={img_col:\"image_path\", lbl_col:\"label\"})\n","    df = df[df[\"image_path\"].apply(lambda p: isinstance(p, str) and os.path.exists(p))]\n","    df = df.dropna(subset=[\"label\"])\n","    df[\"label\"] = df[\"label\"].astype(float).round().astype(int)\n","    if feature in BINARY_MAP:\n","        df[\"label\"] = df[\"label\"].map(BINARY_MAP[feature])\n","        df = df.dropna(subset=[\"label\"]).astype({\"label\": int})\n","    allowed = FEATURE_LABELS.get(feature, DEFAULT_LABELS)\n","    df = df[df[\"label\"].isin(allowed)].reset_index(drop=True)\n","    return df\n","\n","def load_feature_df_multi(csv_paths, feature):\n","    frames = [load_feature_df_one(p, feature) for p in csv_paths]\n","    df = pd.concat(frames, ignore_index=True).drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n","    return df\n","\n","def remove_patient_overlap(df_trainpool, df_test):\n","    overlap = set(df_trainpool.patient_id.unique()) & set(df_test.patient_id.unique())\n","    if overlap:\n","        print(f\" Removing {len(overlap)} patient overlap(s).\")\n","        df_trainpool = df_trainpool[~df_trainpool.patient_id.isin(overlap)].reset_index(drop=True)\n","    return df_trainpool\n","\n","def make_val_from_trainpool(df_trainpool, val_frac=VAL_FRAC_WITHIN_TRAIN, seed=42):\n","    y = df_trainpool[\"label\"].astype(int).values\n","    groups = df_trainpool[\"patient_id\"].values\n","    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n","    N = len(df_trainpool); best = None\n","    for tr_idx, va_idx in sgkf.split(df_trainpool, y=y, groups=groups):\n","        frac = len(va_idx) / N\n","        cand = (abs(frac - val_frac), (tr_idx, va_idx))\n","        if best is None or cand[0] < best[0]: best = cand\n","    tr_idx, va_idx = best[1]\n","    df_tr = df_trainpool.iloc[tr_idx].reset_index(drop=True)\n","    df_va = df_trainpool.iloc[va_idx].reset_index(drop=True)\n","\n","    # safety: ensure all classes appear in val if possible\n","    want = set(df_trainpool[\"label\"].unique())\n","    have = set(df_va[\"label\"].unique())\n","    missing = list(want - have)\n","    if missing:\n","        for cls in missing:\n","            cand = df_tr[df_tr[\"label\"] == cls]\n","            if len(cand):  # move a couple\n","                take_idx = cand.sample(min(2, len(cand)), random_state=seed).index\n","                df_va = pd.concat([df_va, df_tr.loc[take_idx]], ignore_index=True)\n","                df_tr = df_tr.drop(take_idx).reset_index(drop=True)\n","    return df_tr, df_va"],"metadata":{"id":"zd9jGHTiGOWH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# chached version note\n","\n","#•  existing .npy files are the “single” variant (no TTA). The function above will read them as-is when tta=False.\n","#• When tta=True, it will compute once, save as tta10 (or tta5), and reuse next runs.\n","\n","\n","#the frozen GDF path will (a) reuse your existing single-view cache, and (b) build & reuse a separate TTA-averaged cache the first time you run with TTA."],"metadata":{"id":"kOieI6IvKoaN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================================================\n","# Google Derm Foundation (TF) — embedding + TTA\n","# ================================================================\n","# Load once\n","derm_model = from_pretrained_keras(\"google/derm-foundation\")\n","derm_infer = derm_model.signatures[\"serving_default\"]"],"metadata":{"id":"MF0FSeDO5WUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------------- GDF cache helpers ----------------\n","CACHE_DIR = \"/content/drive/MyDrive/Skin_project/derm_emb_cache\"  # <- your cache"],"metadata":{"id":"700GrtS6L68l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------------- GDF cache helpers ----------------\n","CACHE_DIR = \"/content/drive/MyDrive/Skin_project/derm_emb_cache\"  # <- your cache\n","\n","def _sha1(s: str) -> str:\n","    import hashlib\n","    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n","\n","def _emb_cache_path(img_path: str, cache_dir: str) -> str:\n","    return os.path.join(cache_dir, _sha1(img_path) + \".npy\")\n","\n","def _save_embed_to_cache(img_path: str, cache_dir: str, arr: np.ndarray):\n","    os.makedirs(cache_dir, exist_ok=True)\n","    np.save(_emb_cache_path(img_path, cache_dir), arr.astype(np.float32), allow_pickle=False)\n","\n","def _load_embed_if_cached(img_path: str, cache_dir: str):\n","    fp = _emb_cache_path(img_path, cache_dir)\n","    if os.path.exists(fp):\n","        try:\n","            arr = np.load(fp, allow_pickle=False)\n","            if arr.ndim == 1 and arr.dtype == np.float32:\n","                return arr\n","        except Exception:\n","            pass\n","    return None\n","\n","# Lazy init of GDF TF model (only if we must infer)\n","derm_model = None\n","derm_infer = None\n","def _ensure_gdf_loaded():\n","    global derm_model, derm_infer\n","    if derm_infer is None:\n","        from huggingface_hub import from_pretrained_keras\n","        derm_model = from_pretrained_keras(\"google/derm-foundation\")\n","        derm_infer = derm_model.signatures[\"serving_default\"]\n","\n","@tf.function\n","def _gdf_embed_batch(examples):\n","    outs = derm_infer(inputs=examples)\n","    return outs[\"embedding\"]  # [B, 6144]\n","\n","def _png_bytes_from_pil(pil_img: Image.Image) -> bytes:\n","    import io\n","    buf = io.BytesIO()\n","    pil_img.convert(\"RGB\").save(buf, format=\"PNG\")\n","    return buf.getvalue()\n","\n","def _tf_example_from_png_bytes(image_bytes: bytes) -> bytes:\n","    ex = tf.train.Example(features=tf.train.Features(\n","        feature={'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))}\n","    ))\n","    return ex.SerializeToString()\n","\n","def gdf_embeddings_cached(paths, cache_dir, allow_infer=False, tta=False, batch=96):\n","    \"\"\"\n","    Returns (X, keep_idx). If allow_infer=False, only cached vectors are used.\n","    If allow_infer=True and any are missing, they are computed and cached.\n","    If tta=True, we compute/average multiple crops (requires inference).\n","    \"\"\"\n","    embs = []\n","    missing = []\n","    for p in paths:\n","        e = _load_embed_if_cached(p, cache_dir)\n","        if e is None:\n","            missing.append(p)\n","            embs.append(None)\n","        else:\n","            embs.append(e)\n","\n","    if (tta or allow_infer) and len(missing) > 0:\n","        # TTA or fill missing requires inference\n","        _ensure_gdf_loaded()\n","        to_fill = {}\n","        for p in missing:\n","            try:\n","                img = Image.open(p)\n","            except Exception:\n","                to_fill[p] = None\n","                continue\n","            pils = [_ for _ in [img.convert(\"RGB\")] ] if not tta else _gdf_tta_variants(img, use_hflip=True)\n","            exs  = [_tf_example_from_png_bytes(_png_bytes_from_pil(pi)) for pi in pils]\n","            vecs = []\n","            for i in range(0, len(exs), batch):\n","                sub = tf.constant(exs[i:i+batch])\n","                v = _gdf_embed_batch(sub).numpy().astype(np.float32)\n","                vecs.append(v)\n","            vecs = np.concatenate(vecs, 0) if vecs else np.zeros((0,6144), np.float32)\n","            to_fill[p] = vecs.mean(0) if len(vecs) else None\n","\n","        # write back and cache\n","        for i, p in enumerate(paths):\n","            if embs[i] is None and p in to_fill and to_fill[p] is not None:\n","                embs[i] = to_fill[p]\n","                _save_embed_to_cache(p, cache_dir, embs[i])\n","\n","    # collect valid\n","    keep = [i for i, e in enumerate(embs) if e is not None]\n","    if len(keep) < len(paths):\n","        print(f\" GDF cache: using {len(keep)}/{len(paths)} embeddings \"\n","              f\"({'no inference' if not (tta or allow_infer) else 'some inferred'})\")\n","    X = np.stack([embs[i] for i in keep], 0) if keep else np.zeros((0,6144), np.float32)\n","    return X, keep\n"],"metadata":{"id":"KV5BFVE579VJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- add this helper somewhere above run_frozen_lr_backbone ---\n","import os, hashlib, numpy as np\n","\n","def _sha1(s: str) -> str:\n","    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n","\n","def gdf_embeddings_from_cache(paths, cache_dir):\n","    \"\"\"Return X (N_hit, 6144) and ok_indices (into original 'paths').\"\"\"\n","    ok_idx, feats = [], []\n","    for i, p in enumerate(paths):\n","        fp = os.path.join(cache_dir, _sha1(p) + \".npy\")\n","        if os.path.exists(fp):\n","            try:\n","                a = np.load(fp, allow_pickle=False)\n","                if a.ndim == 1 and a.shape[0] == 6144:\n","                    feats.append(a.astype(np.float32))\n","                    ok_idx.append(i)\n","            except Exception:\n","                pass\n","    X = np.stack(feats, axis=0) if feats else np.zeros((0, 6144), np.float32)\n","    return X, ok_idx\n"],"metadata":{"id":"wWHELUxWBpCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _png_bytes_from_pil(pil_img: Image.Image) -> bytes:\n","    buf = io.BytesIO()\n","    pil_img.convert(\"RGB\").save(buf, format=\"PNG\")\n","    return buf.getvalue()\n","\n","def _tf_example_from_png_bytes(image_bytes: bytes) -> bytes:\n","    ex = tf.train.Example(features=tf.train.Features(\n","        feature={'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))}\n","    ))\n","    return ex.SerializeToString()\n","\n","@tf.function\n","def _gdf_embed_batch(examples):\n","    outs = derm_infer(inputs=examples)\n","    return outs[\"embedding\"]  # [B, 6144]\n","\n","def _gdf_tta_variants(pil_img, use_hflip=True):\n","    \"\"\"Small TTA for GDF: FiveCrop-style without requiring size, plus optional hflip.\"\"\"\n","    # make a square from center, then produce 5 crops (TL,TR,BL,BR,center)\n","    im = pil_img.convert(\"RGB\")\n","    w,h = im.size; side = min(w,h); l=(w-side)//2; t=(h-side)//2\n","    sq = im.crop((l,t,l+side,t+side))\n","    # define 5 crops by percentages\n","    s = side\n","    c = s//2\n","    crops = [\n","        sq.crop((0,0,c,c)),               # TL\n","        sq.crop((s-c,0,s,c)),             # TR\n","        sq.crop((0,s-c,c,s)),             # BL\n","        sq.crop((s-c,s-c,s,s)),           # BR\n","        sq.crop(((s-c)//2,(s-c)//2,(s+c)//2,(s+c)//2)),  # center\n","    ]\n","    if use_hflip:\n","        crops += [TF.hflip(c) for c in crops]\n","    return crops  # list of PIL\n","\n","\n","def gdf_embeddings(paths, tta=True, batch=96, use_cache=True, allow_infer=False, hflip=True):\n","    \"\"\"\n","    Return per-image GDF embeddings.\n","    - If tta=False  → use (or build) 'single' cache (compatible with your existing cache).\n","    - If tta=True   → average over FiveCrop(+flip) and save under variant key like 'tta10'.\n","    - use_cache=True: try cache first; allow_infer controls whether we compute missing ones.\n","    \"\"\"\n","    embs = []\n","    variant = \"tta10\" if tta and hflip else (\"tta5\" if tta else \"single\")\n","\n","    for p in paths:\n","        # 1) try cache\n","        if use_cache:\n","            e_cached = _load_embed_if_cached(p, variant=variant)\n","            if e_cached is not None:\n","                embs.append(e_cached); continue\n","            if (variant != \"single\"):  # fallback to single if you had only single cached\n","                e_single = _load_embed_if_cached(p, variant=\"single\")\n","                if e_single is not None and not allow_infer:\n","                    # if we’re not allowed to infer, at least return the single-view\n","                    embs.append(e_single); continue\n","\n","        # 2) compute if allowed\n","        if not allow_infer:\n","            embs.append(None); continue\n","\n","        try:\n","            img = Image.open(p)\n","        except Exception:\n","            embs.append(None); continue\n","\n","        if tta:\n","            pils = _gdf_tta_variants(img, use_hflip=hflip)  # ~10 views\n","        else:\n","            pils = [img.convert(\"RGB\")]\n","\n","        # encode to tf.Example\n","        exs = [_tf_example_from_png_bytes(_png_bytes_from_pil(pi)) for pi in pils]\n","\n","        # batched TF inference (CPU)\n","        E = []\n","        for i in range(0, len(exs), batch):\n","            sub = tf.constant(exs[i:i+batch])\n","            e = _gdf_embed_batch(sub).numpy().astype(np.float32)  # [b,6144]\n","            E.append(e)\n","        E = np.concatenate(E, axis=0) if E else np.zeros((0, 6144), np.float32)\n","        e_mean = E.mean(0) if len(E) else None\n","\n","        embs.append(e_mean)\n","        # save to cache\n","        if e_mean is not None and use_cache:\n","            _save_embed_to_cache(p, e_mean, variant=variant)\n","\n","    ok = [i for i,v in enumerate(embs) if v is not None]\n","    X  = np.stack([embs[i] for i in ok], axis=0) if ok else np.zeros((0,6144), np.float32)\n","    return X, ok\n","\n","\n","# ================================================================\n","# timm encoders (PanDerm/DINOv2) — embedding + TTA\n","# ================================================================\n","def _timm_name(backbone: str):\n","    if backbone == \"panderm_b16\": return \"vit_base_patch16_224\"\n","    if backbone == \"panderm_l16\": return \"vit_large_patch16_224\"\n","    if backbone == \"dinov2_b14\":  return \"vit_base_patch14_dinov2\"\n","    if backbone == \"dinov2_l14\":  return \"vit_large_patch14_dinov2\"\n","    raise ValueError(f\"Unknown backbone: {backbone}\")\n","\n","def _interpolate_pos_embed(model, posemb):\n","    pe_new = model.pos_embed\n","    if posemb.shape == pe_new.shape: return posemb\n","    cls_tok, grid = posemb[:, :1], posemb[:, 1:]\n","    cls_tok_new, grid_new = pe_new[:, :1], pe_new[:, 1:]\n","    gs_old = int(grid.shape[1] ** 0.5)\n","    gs_new = int(grid_new.shape[1] ** 0.5)\n","    grid = grid.reshape(1, gs_old, gs_old, -1).permute(0,3,1,2)\n","    grid = torch.nn.functional.interpolate(grid, size=(gs_new, gs_new), mode=\"bicubic\", align_corners=False)\n","    grid = grid.permute(0,2,3,1).reshape(1, gs_new*gs_new, -1)\n","    return torch.cat([cls_tok_new, grid], dim=1)\n","\n","def load_panderm_backbone(encoder, ckpt_path, verbose=True):\n","    sd_raw = torch.load(ckpt_path, map_location=\"cpu\")\n","    sd = sd_raw.get(\"model\") or sd_raw.get(\"state_dict\") or sd_raw\n","    def strip(k):\n","        for p in (\"module.\",\"backbone.\",\"encoder.\",\"model.\"):\n","            if k.startswith(p): return k[len(p):]\n","        return k\n","    enc_sd = encoder.state_dict()\n","    mapped = {}\n","    for k,v in sd.items():\n","        k2 = strip(k)\n","        if k2 == \"pos_embed\" and \"pos_embed\" in enc_sd:\n","            try: v = _interpolate_pos_embed(encoder, v)\n","            except Exception: pass\n","        if k2 in enc_sd and enc_sd[k2].shape == v.shape:\n","            mapped[k2] = v\n","    encoder.load_state_dict(mapped, strict=False)\n","    if verbose:\n","        miss = set(enc_sd.keys()) - set(mapped.keys())\n","        print(f\"PanDerm load: matched {len(mapped)}/{len(enc_sd)}; missing {len(miss)}\")\n","\n","def build_timm_encoder(backbone, PANDERM_CKPT=None):\n","    name = _timm_name(backbone)\n","    if backbone.startswith(\"dinov2\"):\n","        enc = timm.create_model(name, pretrained=True, num_classes=0)\n","    else:\n","        assert PANDERM_CKPT and os.path.exists(PANDERM_CKPT), \"Set PANDERM_CKPT\"\n","        enc = timm.create_model(name, pretrained=False, num_classes=0)\n","        load_panderm_backbone(enc, PANDERM_CKPT, verbose=True)\n","    enc.eval().to(DEVICE)\n","    for p in enc.parameters(): p.requires_grad = False\n","\n","    cfg = resolve_data_config({}, model=enc)\n","    mean, std = tuple(cfg.get(\"mean\",(0.485,0.456,0.406))), tuple(cfg.get(\"std\",(0.229,0.224,0.225)))\n","    size = cfg.get(\"input_size\",(3,224,224))[-1]\n","\n","    pre  = transforms.Resize(int(size*1.5))\n","    five = transforms.FiveCrop(size)\n","    post = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","    eval_tfm = transforms.Compose([transforms.Resize(int(size*1.5)), transforms.CenterCrop(size), post])\n","\n","    return enc, (pre,five,post), eval_tfm\n","\n","@torch.no_grad()\n","def timm_embeddings(paths, encoder, eval_tfm, batch=32, num_workers=2):\n","    class _DS(Dataset):\n","        def __init__(self, paths, tfm): self.paths=list(paths); self.tfm=tfm\n","        def __len__(self): return len(self.paths)\n","        def __getitem__(self, i):\n","            x = Image.open(self.paths[i]).convert(\"RGB\"); return self.tfm(x)\n","    ds = _DS(paths, eval_tfm)\n","    dl = DataLoader(ds, batch_size=batch, shuffle=False, num_workers=num_workers, pin_memory=True)\n","    chunks=[]\n","    for xb in dl:\n","        xb = xb.to(DEVICE, non_blocking=True)\n","        z = encoder(xb)\n","        if z.dim()>2: z=z.mean((2,3))\n","        chunks.append(z.detach().cpu().numpy())\n","    return np.concatenate(chunks,0) if chunks else np.zeros((0, encoder.num_features), np.float32)\n","\n","@torch.no_grad()\n","def timm_embeddings_tta(paths, encoder, tta_tuple, hflip=True, micro_batch=8):\n","    pre, five, post = tta_tuple\n","    feats=[]\n","    for p in paths:\n","        try: img = Image.open(p).convert(\"RGB\")\n","        except Exception: feats.append(None); continue\n","        img = pre(img)\n","        crops = list(five(img))\n","        if hflip: crops += [TF.hflip(c) for c in crops]\n","        tens = torch.stack([post(c) for c in crops],0)  # [T,3,H,W]\n","        zs=[]\n","        for i in range(0, tens.size(0), micro_batch):\n","            xb = tens[i:i+micro_batch].to(DEVICE)\n","            z = encoder(xb)\n","            if z.dim()>2: z=z.mean((2,3))\n","            zs.append(z.detach().cpu())\n","        zm = torch.cat(zs,0).mean(0).numpy()\n","        feats.append(zm)\n","    ok = [i for i,v in enumerate(feats) if v is not None]\n","    X  = np.stack([feats[i] for i in ok],0) if ok else np.zeros((0, encoder.num_features), np.float32)\n","    return X, ok"],"metadata":{"id":"oiTlkfMLHLre"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================================================\n","# LR pipeline + metrics\n","# ================================================================\n","def choose_pca_components(n_train, n_feat, keep=256):\n","    cap = min(n_feat, max(0, n_train-1), keep)\n","    return cap if cap >= 2 else 0\n","\n","def make_lr_pipeline(n_comp, C=1.0, whiten=True, seed=42):\n","    if n_comp and n_comp>=2:\n","        return make_pipeline(\n","            StandardScaler(with_mean=True, with_std=True),\n","            PCA(n_components=n_comp, whiten=whiten, random_state=seed),\n","            LogisticRegression(solver=\"lbfgs\", class_weight=\"balanced\", C=C, max_iter=300, random_state=seed),\n","        )\n","    else:\n","        return make_pipeline(\n","            StandardScaler(with_mean=True, with_std=True),\n","            LogisticRegression(solver=\"lbfgs\", class_weight=\"balanced\", C=C, max_iter=300, random_state=seed),\n","        )\n","\n","def tune_threshold_binary(scores, y_true, grid=np.linspace(0.2,0.8,31)):\n","    best_t, best_f1 = 0.5, -1.0\n","    for t in grid:\n","        pred = (scores >= t).astype(int)\n","        f1 = f1_score(y_true, pred, average=\"macro\", zero_division=0)\n","        if f1 > best_f1:\n","            best_f1, best_t = f1, t\n","    return float(best_t)\n","\n","def _metrics_binary(y_true, y_pred, scores):\n","    return dict(\n","        macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n","        weighted_f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n","        acc = accuracy_score(y_true, y_pred),\n","        mcc = matthews_corrcoef(y_true, y_pred),\n","        qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\"),\n","        auroc = roc_auc_score(y_true, scores),\n","    )\n","\n","\n","def _metrics_multiclass(y_true, y_pred, proba, label_order=None):\n","    out = dict(\n","        macro_f1   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n","        weighted_f1= f1_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n","        acc        = accuracy_score(y_true, y_pred),\n","        mcc        = matthews_corrcoef(y_true, y_pred),\n","        qwk        = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\"),\n","        auroc_micro = roc_auc_score(y_true, proba, multi_class=\"ovr\", average=\"micro\",labels=label_order),\n","        auroc_macro = roc_auc_score(y_true, proba, multi_class=\"ovr\", average=\"macro\",labels=label_order),\n","    )\n","    # OPTIONAL (comment out if you don’t want them)\n","    # out[\"top2\"]        = top_k_accuracy_score(y_true, proba, k=2, labels=label_order)\n","    # out[\"auroc_micro\"] = roc_auc_score(y_true, proba, multi_class=\"ovr\", average=\"micro\", labels=label_order)\n","    # out[\"auroc_macro\"] = roc_auc_score(y_true, proba, multi_class=\"ovr\", average=\"macro\", labels=label_order)\n","    return out\n","\n","\n","# ================================================================\n","# Main runner (frozen + LR) for one backbone/feature\n","# ================================================================\n","\n","def run_frozen_lr_backbone(\n","    backbone,               # \"gdf\" | \"panderm_b16\" | \"panderm_l16\" | \"dinov2_b14\" | \"dinov2_l14\"\n","    feature,\n","    use_synthetic=False,\n","    PANDERM_CKPT=None,\n","    C_grid=(1.0,),          # try (0.5, 1.0, 2.0)\n","    pca_keep=256,\n","    do_tta=True,\n","    seed=42,\n","    verbose_report=True,\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False, # infer & cache any missing\n","):\n","    # -- split data\n","    pools = [CSV_TRAIN_POOL] + ([CSV_SYN_WIDE] if use_synthetic else [])\n","    df_pool = load_feature_df_multi(pools, feature)\n","    df_test = load_feature_df_one(CSV_TEST_FIXED, feature)\n","    df_pool = remove_patient_overlap(df_pool, df_test)\n","    df_tr, df_va = make_val_from_trainpool(df_pool, val_frac=VAL_FRAC_WITHIN_TRAIN, seed=seed)\n","\n","    classes = np.array(sorted(FEATURE_LABELS.get(feature, DEFAULT_LABELS)))\n","    lab2idx = {int(l): i for i,l in enumerate(classes)}\n","    nC = len(classes); is_binary = (nC==2)\n","\n","    # -- embeddings\n","    if backbone == \"gdf\":\n","        #X_tr,_ = gdf_embeddings(df_tr.image_path.values, tta=do_tta)\n","        #X_va,_ = gdf_embeddings(df_va.image_path.values, tta=do_tta)\n","        #X_te,_ = gdf_embeddings(df_test.image_path.values, tta=do_tta)\n","\n","        # cache-first path\n","        #X_tr, keep_tr = gdf_embeddings_cached(df_tr.image_path.values, gdf_cache_dir,\n","                                              #allow_infer=gdf_allow_infer, tta=do_tta)\n","        #X_va, keep_va = gdf_embeddings_cached(df_va.image_path.values, gdf_cache_dir,\n","                                              #allow_infer=gdf_allow_infer, tta=do_tta)\n","        #X_te, keep_te = gdf_embeddings_cached(df_test.image_path.values, gdf_cache_dir,\n","                                             # allow_infer=gdf_allow_infer, tta=do_tta)\n","\n","        # align labels to kept indices\n","        #y_tr = df_tr.label.values.astype(int)[keep_tr]\n","        #y_va = df_va.label.values.astype(int)[keep_va]\n","        #y_te = df_test.label.values.astype(int)[keep_te]\n","\n","\n","        # cache-only, no TF calls\n","        X_tr, ok_tr = gdf_embeddings_from_cache(df_tr.image_path.values, gdf_cache_dir)\n","        X_va, ok_va = gdf_embeddings_from_cache(df_va.image_path.values, gdf_cache_dir)\n","        X_te, ok_te = gdf_embeddings_from_cache(df_test.image_path.values, gdf_cache_dir)\n","\n","        # align labels to hits\n","        y_tr = df_tr.label.values.astype(int)[ok_tr]\n","        y_va = df_va.label.values.astype(int)[ok_va]\n","        y_te = df_test.label.values.astype(int)[ok_te]\n","\n","        print(f\"[GDF cache] hits  train {len(ok_tr)}/{len(df_tr)} | \"\n","              f\"val {len(ok_va)}/{len(df_va)} | test {len(ok_te)}/{len(df_test)}\")\n","\n","        # Safety check: abort early if we have almost no data\n","        if len(X_tr) < 5 or len(np.unique(y_tr)) < 2:\n","            raise RuntimeError(\"Too few cached embeddings or single-class train fold. \"\n","                              \"Check cache paths or enable gdf_allow_infer=True to fill misses.\")\n","\n","    else:\n","        enc, tta_tuple, eval_tfm = build_timm_encoder(backbone, PANDERM_CKPT=PANDERM_CKPT)\n","        if do_tta:\n","            X_tr,_ = timm_embeddings_tta(df_tr.image_path.values,  enc, tta_tuple, hflip=True)\n","            X_va,_ = timm_embeddings_tta(df_va.image_path.values,  enc, tta_tuple, hflip=True)\n","            X_te,_ = timm_embeddings_tta(df_test.image_path.values, enc, tta_tuple, hflip=True)\n","        else:\n","            X_tr   = timm_embeddings(df_tr.image_path.values,  enc, eval_tfm)\n","            X_va   = timm_embeddings(df_va.image_path.values,  enc, eval_tfm)\n","            X_te   = timm_embeddings(df_test.image_path.values, enc, eval_tfm)\n","\n","    y_tr = df_tr.label.values.astype(int)\n","    y_va = df_va.label.values.astype(int)\n","    y_te = df_test.label.values.astype(int)\n","\n","    # -- train LR (grid on C; PCA safe cap)\n","    best = None\n","    for C in C_grid:\n","        n_comp = choose_pca_components(len(X_tr), X_tr.shape[1], keep=pca_keep)\n","        clf = make_lr_pipeline(n_comp, C=C, whiten=True, seed=seed)\n","        clf.fit(X_tr, y_tr)\n","        va_pred = clf.predict(X_va)\n","        va_f1   = f1_score(y_va, va_pred, average=\"macro\", zero_division=0)\n","        if (best is None) or (va_f1 > best[\"va_f1\"]):\n","            best = {\"C\": C, \"va_f1\": va_f1, \"clf\": clf, \"n_comp\": n_comp}\n","    clf = best[\"clf\"]\n","\n","    report = {\"backbone\": backbone, \"feature\": feature, \"use_synth\": bool(use_synthetic),\n","              \"C\": best[\"C\"], \"pca_comp\": best[\"n_comp\"], \"val_macroF1\": float(best[\"va_f1\"])}\n","\n","    # -- evaluate (threshold-tuned for binary)\n","    if is_binary:\n","        va_scores = clf.predict_proba(X_va)[:,1]\n","        thr = tune_threshold_binary(va_scores, y_va)\n","        te_scores = clf.predict_proba(X_te)[:,1]\n","        y_pred = (te_scores >= thr).astype(int)\n","        metrics = _metrics_binary(y_te, y_pred, te_scores)\n","        report.update(metrics)\n","        report[\"threshold\"] = float(thr)\n","        if verbose_report:\n","            print(f\"\\n[{backbone} · {feature} · frozen+LR{' + TTA' if do_tta else ''}]\")\n","            print(f\"C={best['C']}  PCA={best['n_comp']}  thr={thr:.3f}  val_macroF1={best['va_f1']:.3f}\")\n","            print(f\"macro-F1={metrics['macro_f1']:.3f} | weighted-F1={metrics['weighted_f1']:.3f} | \"\n","                  f\"acc={metrics['acc']:.3f} | MCC={metrics['mcc']:.3f} | QWK={metrics['qwk']:.3f} | \"\n","                  f\"AUROC={metrics['auroc']:.3f}\")\n","    else:\n","        # --- MULTICLASS EVAL  ---\n","        lr_step     = clf.named_steps.get(\"logisticregression\", clf[-1])\n","        classes_lr  = lr_step.classes_                     # e.g., array([-1, 0, 1])\n","        P           = clf.predict_proba(X_te)              # columns are ordered as classes_lr\n","        y_pred      = classes_lr[P.argmax(1)]              # map indices -> label values\n","\n","        metrics = _metrics_multiclass(y_te, y_pred, P, label_order=classes_lr)\n","        report.update(metrics)\n","\n","        if verbose_report:\n","            present = np.unique(y_te)\n","            print(f\"\\n[{backbone} · {feature} · frozen+LR{' + TTA' if do_tta else ''}]\")\n","            print(f\"C={best['C']}  PCA={best['n_comp']}  val_macroF1={best['va_f1']:.3f}\")\n","            print(f\"macro-F1={metrics['macro_f1']:.3f} | weighted-F1={metrics['weighted_f1']:.3f} | \"\n","                  f\"acc={metrics['acc']:.3f} | MCC={metrics['mcc']:.3f} | QWK={metrics['qwk']:.3f}\")\n","            print(classification_report(y_te, y_pred, labels=present, digits=3, zero_division=0))\n","            print(\"Confusion (rows=true, cols=pred):\\n\", confusion_matrix(y_te, y_pred, labels=present))\n","\n","    # free a bit\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","    gc.collect()\n","    return report\n","\n","# ================================================================\n","# Dummy baselines (train labels -> test predictions)\n","# ================================================================\n","def dummy_baselines_from_labels(y_train, y_test):\n","    X_tr = np.zeros((len(y_train),1)); X_te = np.zeros((len(y_test),1))\n","    out = {}\n","    for strat in [\"most_frequent\", \"uniform\"]:\n","        d = DummyClassifier(strategy=strat, random_state=GLOBAL_SEED).fit(X_tr, y_train)\n","        yp = d.predict(X_te)\n","        out[strat] = dict(\n","            macro_f1 = f1_score(y_test, yp, average=\"macro\", zero_division=0),\n","            bal_acc  = (confusion_matrix(y_test, yp, labels=np.unique(y_test)).astype(float)\n","                        / confusion_matrix(y_test, yp, labels=np.unique(y_test)).sum(1, keepdims=True)\n","                       ).diagonal().mean()\n","        )\n","    return out\n","\n","# small helper to run baselines on current splits\n","def run_dummy_for_split(df_tr, df_va, df_te):\n","    y_tr = df_tr.label.values.astype(int)\n","    y_te = df_te.label.values.astype(int)\n","    return dummy_baselines_from_labels(y_tr, y_te)\n"],"metadata":{"id":"JRRp0k_cAao-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ================================================================\n","# Repeat-N wrapper (seed sweep) with mean±std\n","# ================================================================\n","\n","def repeat_avg_frozen(backbone, feature, n_runs=3, base_seed=2025,\n","                      use_synthetic=True, PANDERM_CKPT=None, C_grid=(1.0,), do_tta=True, #GDF:\n","                      gdf_cache_dir=CACHE_DIR, gdf_allow_infer=False):\n","    rows=[]\n","    for i in range(n_runs):\n","        s = base_seed + i\n","        random.seed(s); np.random.seed(s); torch.manual_seed(s)\n","        if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n","        rep = run_frozen_lr_backbone(\n","            backbone=backbone, feature=feature, use_synthetic=use_synthetic,\n","            PANDERM_CKPT=PANDERM_CKPT, C_grid=C_grid, do_tta=do_tta, seed=s, verbose_report=False,\n","            gdf_cache_dir=gdf_cache_dir, gdf_allow_infer=gdf_allow_infer  # <-- GDF\n","        )\n","        rows.append(rep)\n","    df = pd.DataFrame(rows)\n","    means = df.mean(numeric_only=True)\n","    stds  = df.std(numeric_only=True)\n","    summary = {k: float(means[k]) for k in means.index}\n","    summary.update({f\"{k}_std\": float(stds[k]) for k in means.index})\n","    summary.update(dict(backbone=backbone, feature=feature, n_runs=n_runs, use_synth=bool(use_synthetic)))\n","    print(f\"\\n[{backbone} · {feature} · frozen+LR{' + TTA' if do_tta else ''} | {n_runs} runs]\"\n","          f\" macro-F1={summary.get('macro_f1', np.nan):.3f}±{summary.get('macro_f1_std', np.nan):.3f}\"\n","          f\" | acc={summary.get('acc', np.nan):.3f}±{summary.get('acc_std', np.nan):.3f}\"\n","          f\" | weighted-F1={summary.get('weighted_f1', np.nan):.3f}±{summary.get('weighted_f1_std', np.nan):.3f}\")\n","    return df, summary"],"metadata":{"id":"zZB8QTdmAamX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================================================\n","# How to run (examples)\n","# ================================================================\n","# 1) Real-only vs Real+Synth comparisons\n","#    (a) GDF (frozen)\n","# rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","#     backbone=\"gdf\", feature=\"hyperpigmentation\",\n","#     n_runs=3, base_seed=2025,\n","#     use_synthetic=False,   # then flip to True to compare\n","#     C_grid=(0.5,1.0,2.0), do_tta=True\n","# )\n","\n","# 2) PanDerm (frozen) — supply checkpoint\n","# PANDERM_CKPT = \"/content/panderm_weights/panderm_bb_data6_checkpoint-499.pth\"\n","# rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","#     backbone=\"panderm_b16\", feature=\"texture\",\n","#     n_runs=3, base_seed=2025,\n","#     use_synthetic=True,\n","#     PANDERM_CKPT=PANDERM_CKPT,\n","#     C_grid=(0.5,1.0,2.0), do_tta=True\n","# )\n","\n","# 3) DINOv2 (frozen)\n","# rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","#     backbone=\"dinov2_b14\", feature=\"elasticity\",\n","#     n_runs=3, base_seed=2025,\n","#     use_synthetic=True,\n","#     C_grid=(0.5,1.0,2.0), do_tta=True\n","# )\n","\n","# 4) Quick per-run detailed report (not averaged)\n","# _ = run_frozen_lr_backbone(\"gdf\", \"oiliness\", use_synthetic=False, C_grid=(1.0,1.5,2.0), do_tta=True)\n","\n","# ================================================================\n","# NOTE\n","# - This file standardizes *frozen* comparisons. Keep your PanDerm/DINOv2 **fine-tune**\n","#   notebook (StrongHead + Cosine + TTA) separate, using the same metrics printing.\n","# - For fairness, you can also add *frozen* LR runs of PanDerm/DINOv2 alongside GDF.\n","# ================================================================\n"],"metadata":{"id":"f9XDa7iDAakj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **GDF**"],"metadata":{"id":"5NXKrOhg9pYd"}},{"cell_type":"markdown","source":["## 5. Hyperpigmentation"],"metadata":{"id":"4vo9cE6F9meQ"}},{"cell_type":"markdown","source":["## Real only"],"metadata":{"id":"t8k5VgYSFoYU"}},{"cell_type":"code","source":["# GDF - CACHED, NO INFERENCE\n","rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","    backbone=\"gdf\",\n","    feature=\"hyperpigmentation\",\n","    n_runs=3, base_seed=2025,\n","    use_synthetic=False,\n","    C_grid=(0.5,1.0,2.0),\n","    do_tta=False,                 # keep False to stay cache-only\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False\n",")\n"],"metadata":{"id":"kh67S4zV9drw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### with Synthetic Data"],"metadata":{"id":"_7rXZT6KFqhA"}},{"cell_type":"code","source":["# GDF - CACHED, NO INFERENCE\n","rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","    backbone=\"gdf\",\n","    feature=\"hyperpigmentation\",\n","    n_runs=3, base_seed=2025,\n","    use_synthetic=True,\n","    C_grid=(0.5,1.0,2.0),\n","    do_tta=False,\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False         # <-- crucial: do not touch TF model\n",")\n"],"metadata":{"id":"wky7VHXK_Inz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Moisture"],"metadata":{"id":"RW_yP5EEFweg"}},{"cell_type":"markdown","source":["### Real only"],"metadata":{"id":"NtP8lT4KF0vP"}},{"cell_type":"code","source":["# GDF - CACHED, NO INFERENCE\n","rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","    backbone=\"gdf\",\n","    feature=\"moisture\",\n","    n_runs=3, base_seed=2025,\n","    use_synthetic=False,\n","    C_grid=(0.5,1.0,2.0),\n","    do_tta=False,                 # keep False to stay cache-only\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False\n",")"],"metadata":{"id":"hGo68tQwAagY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### with Synth data"],"metadata":{"id":"F5KqXl7FGRsj"}},{"cell_type":"code","source":["# GDF - CACHED, NO INFERENCE\n","rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","    backbone=\"gdf\",\n","    feature=\"moisture\",\n","    n_runs=3, base_seed=2025,\n","    use_synthetic=True,\n","    C_grid=(0.5,1.0,2.0),\n","    do_tta=False,                 # keep False to stay cache-only\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False\n",")"],"metadata":{"id":"0U6nDtInAael"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Oiliness"],"metadata":{"id":"-GelBxm6GhUS"}},{"cell_type":"markdown","source":["### Real only"],"metadata":{"id":"k7RYE8GFGjXA"}},{"cell_type":"code","source":["# GDF - CACHED, NO INFERENCE\n","rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","    backbone=\"gdf\",\n","    feature=\"oiliness\",\n","    n_runs=3, base_seed=2025,\n","    use_synthetic=False,\n","    C_grid=(0.5,1.0,2.0),\n","    do_tta=False,                 # keep False to stay cache-only\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False\n",")"],"metadata":{"id":"-pa8uplHAacv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With Synth data"],"metadata":{"id":"esIGodWhG5wX"}},{"cell_type":"code","source":["# GDF - CACHED, NO INFERENCE\n","rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","    backbone=\"gdf\",\n","    feature=\"oiliness\",\n","    n_runs=3, base_seed=2025,\n","    use_synthetic=True,\n","    C_grid=(0.5,1.0,2.0),\n","    do_tta=False,                 # keep False to stay cache-only\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False\n",")"],"metadata":{"id":"2ZuF4HdcAaaZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Elasticity"],"metadata":{"id":"haauSSxaHEBG"}},{"cell_type":"markdown","source":["### Real only"],"metadata":{"id":"oazaJ7hIHKOf"}},{"cell_type":"code","source":["# GDF - CACHED, NO INFERENCE\n","rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","    backbone=\"gdf\",\n","    feature=\"elasticity\",\n","    n_runs=3, base_seed=2025,\n","    use_synthetic=False,\n","    C_grid=(0.5,1.0,2.0),\n","    do_tta=False,                 # keep False to stay cache-only\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False\n",")"],"metadata":{"id":"gRft7s5-AaXz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With Synth data"],"metadata":{"id":"t20p_MB1HP6E"}},{"cell_type":"code","source":["# GDF - CACHED, NO INFERENCE\n","rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","    backbone=\"gdf\",\n","    feature=\"elasticity\",\n","    n_runs=3, base_seed=2025,\n","    use_synthetic=True,\n","    C_grid=(0.5,1.0,2.0),\n","    do_tta=False,                 # keep False to stay cache-only\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False\n",")"],"metadata":{"id":"pD7AEVK5AaOd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Texture\n","\n"],"metadata":{"id":"F8FyQ80uHpRj"}},{"cell_type":"markdown","source":["### Real only"],"metadata":{"id":"XTMBDOO2HvhT"}},{"cell_type":"code","source":["# GDF - CACHED, NO INFERENCE\n","rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","    backbone=\"gdf\",\n","    feature=\"texture\",\n","    n_runs=3, base_seed=2025,\n","    use_synthetic=False,\n","    C_grid=(0.5,1.0,2.0),\n","    do_tta=False,                 # keep False to stay cache-only\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False\n",")"],"metadata":{"id":"9ieux9eGHSD0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With Synth data"],"metadata":{"id":"VfES4OOmH1aF"}},{"cell_type":"code","source":["# GDF - CACHED, NO INFERENCE\n","rep_gdf_hp, sum_gdf_hp = repeat_avg_frozen(\n","    backbone=\"gdf\",\n","    feature=\"texture\",\n","    n_runs=3, base_seed=2024,\n","    use_synthetic=True,\n","    C_grid=(0.5,1.0,2.0),\n","    do_tta=False,                 # keep False to stay cache-only\n","    gdf_cache_dir=CACHE_DIR,\n","    gdf_allow_infer=False\n",")"],"metadata":{"id":"H9kWIZHIHSB_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **PANDERM**  (Large)"],"metadata":{"id":"tjbe8y83IQDm"}},{"cell_type":"code","source":["%%bash\n","set -e\n","pip -q install gdown\n","mkdir -p /content/panderm_weights\n","\n","# ViT-Large (paper model)\n","gdown --fuzzy \"https://drive.google.com/file/d/1SwEzaOlFV_gBKf2UzeowMC8z9UH7AQbE/view?usp=sharing\" \\\n","  -O /content/panderm_weights/panderm_ll_data6_checkpoint-499.pth\n","\n","# ViT-Base (smaller)\n","gdown --fuzzy \"https://drive.google.com/file/d/17J4MjsZu3gdBP6xAQi_NMDVvH65a00HB/view?usp=sharing\" \\\n","  -O /content/panderm_weights/panderm_bb_data6_checkpoint-499.pth\n","\n","ls -lh /content/panderm_weights\n"],"metadata":{"id":"NmFXS_1EJdTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PANDERM_CKPT = \"/content/panderm_weights/panderm_ll_data6_checkpoint-499.pth\""],"metadata":{"id":"DwWrjCiGJP2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Moisture"],"metadata":{"id":"Diwq_-GlJE2f"}},{"cell_type":"markdown","source":["### Real only"],"metadata":{"id":"lkhHt5j0JGxq"}},{"cell_type":"code","source":["# 2) PanDerm (frozen) — supply checkpoint\n","\n","rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","     backbone=\"panderm_l16\", feature=\"moisture\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=False,\n","     PANDERM_CKPT=PANDERM_CKPT,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"64AknPuRHR_7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With Synth Data"],"metadata":{"id":"HhYZ7kL_KaV4"}},{"cell_type":"code","source":["# 2) PanDerm (frozen) — supply checkpoint\n","\n","rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","     backbone=\"panderm_l16\", feature=\"moisture\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=True,\n","     PANDERM_CKPT=PANDERM_CKPT,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"DPxc7jLDHR90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Oiliness"],"metadata":{"id":"LYxwVet2NoBn"}},{"cell_type":"markdown","source":["### Real only"],"metadata":{"id":"vWFaY_sHNqHw"}},{"cell_type":"code","source":["# 2) PanDerm (frozen) — supply checkpoint\n","\n","rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","     backbone=\"panderm_l16\", feature=\"oiliness\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=False,\n","     PANDERM_CKPT=PANDERM_CKPT,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"AyxCBKoTNsPC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With synth"],"metadata":{"id":"0h_gdZiPNvy4"}},{"cell_type":"code","source":["# 2) PanDerm (frozen) — supply checkpoint\n","\n","rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","     backbone=\"panderm_l16\", feature=\"oiliness\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=True,\n","     PANDERM_CKPT=PANDERM_CKPT,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"ciO4WcL0NxkV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Elasticity"],"metadata":{"id":"HhBU8-HLQAz6"}},{"cell_type":"markdown","source":["### Real only"],"metadata":{"id":"vE1TsF0PQN4H"}},{"cell_type":"code","source":["# 2) PanDerm (frozen) — supply checkpoint\n","\n","rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","     backbone=\"panderm_l16\", feature=\"elasticity\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=False,\n","     PANDERM_CKPT=PANDERM_CKPT,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"W1pvx0jpHR7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With Synth data"],"metadata":{"id":"Hw97kzNkQ0rZ"}},{"cell_type":"code","source":["# 2) PanDerm (frozen) — supply checkpoint\n","\n","rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","     backbone=\"panderm_l16\", feature=\"elasticity\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=True,\n","     PANDERM_CKPT=PANDERM_CKPT,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"OWMU-khFH0_Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Texture"],"metadata":{"id":"EidIvZJZRh_I"}},{"cell_type":"markdown","source":["### Real only"],"metadata":{"id":"_c6apIMnRkbo"}},{"cell_type":"code","source":["# 2) PanDerm (frozen) — supply checkpoint\n","\n","rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","     backbone=\"panderm_l16\", feature=\"texture\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=False,\n","     PANDERM_CKPT=PANDERM_CKPT,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"K3ATkBd2H07W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With Synth data"],"metadata":{"id":"-REiLpq1Rnr8"}},{"cell_type":"code","source":["# 2) PanDerm (frozen) — supply checkpoint\n","\n","rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","     backbone=\"panderm_l16\", feature=\"texture\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=True,\n","     PANDERM_CKPT=PANDERM_CKPT,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"aCEstk2fH05B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Hyperpigmentation"],"metadata":{"id":"gp6DwrjcRrIA"}},{"cell_type":"markdown","source":["### Real only"],"metadata":{"id":"BBbODCdkRv8m"}},{"cell_type":"code","source":["# 2) PanDerm (frozen) — supply checkpoint\n","\n","rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","     backbone=\"panderm_l16\", feature=\"hyperpigmentation\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=False,\n","     PANDERM_CKPT=PANDERM_CKPT,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"k4D3hbixRq1Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With Synth data"],"metadata":{"id":"N0fSImkhRxgF"}},{"cell_type":"code","source":["# 2) PanDerm (frozen) — supply checkpoint\n","\n","rep_pan_tex, sum_pan_tex = repeat_avg_frozen(\n","     backbone=\"panderm_l16\", feature=\"hyperpigmentation\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=True,\n","     PANDERM_CKPT=PANDERM_CKPT,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"UEElXIwERqzc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **DINOv2**"],"metadata":{"id":"GRLLr-JELXW_"}},{"cell_type":"markdown","source":["## 1. Moisture"],"metadata":{"id":"w-69fbakLdDN"}},{"cell_type":"markdown","source":["### Real-only"],"metadata":{"id":"6gfsORttLeFm"}},{"cell_type":"code","source":["# DINOv2 (frozen)\n","rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","     backbone=\"dinov2_b14\", feature=\"moisture\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=False,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"2fHN3l7oRqxW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With synth"],"metadata":{"id":"eS8spoqVMek9"}},{"cell_type":"code","source":["# DINOv2 (frozen)\n","rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","     backbone=\"dinov2_b14\", feature=\"moisture\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=True,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"ub-OV2cJRqvh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Oiliness"],"metadata":{"id":"BvjtWnpaMheH"}},{"cell_type":"markdown","source":["### Real-only"],"metadata":{"id":"2nXQaQtzMlBV"}},{"cell_type":"code","source":["# DINOv2 (frozen)\n","rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","     backbone=\"dinov2_b14\", feature=\"oiliness\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=False,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"Om7YC_nXRqtd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With synth"],"metadata":{"id":"D0vcjhsqMqjB"}},{"cell_type":"code","source":["# DINOv2 (frozen)\n","rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","     backbone=\"dinov2_b14\", feature=\"oiliness\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=True,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"6MDytVluRqrG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Elasticity"],"metadata":{"id":"_zQO7kBHMvog"}},{"cell_type":"markdown","source":["### Real-only"],"metadata":{"id":"U_6U3GJIMytP"}},{"cell_type":"code","source":["# DINOv2 (frozen)\n","rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","     backbone=\"dinov2_b14\", feature=\"elasticity\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=False,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"XAVeAX4ALXFI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With synth"],"metadata":{"id":"uEE8agDxM5QA"}},{"cell_type":"code","source":["# DINOv2 (frozen)\n","rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","     backbone=\"dinov2_b14\", feature=\"elasticity\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=True,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"nL0NZRsQLXDD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Texture"],"metadata":{"id":"Doxe4lFaNF9Z"}},{"cell_type":"markdown","source":["### Real-only"],"metadata":{"id":"Uhuv4zl6NIm6"}},{"cell_type":"code","source":["# DINOv2 (frozen)\n","rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","     backbone=\"dinov2_b14\", feature=\"texture\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=False,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"5w7hHOtFLXBG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With Synth"],"metadata":{"id":"FJfJcP5DNQpg"}},{"cell_type":"code","source":["# DINOv2 (frozen)\n","rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","     backbone=\"dinov2_b14\", feature=\"texture\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=True,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"LqmuoOFPLW_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Hyperpigmentation"],"metadata":{"id":"_ebhD8quNTIY"}},{"cell_type":"markdown","source":["### Real-only"],"metadata":{"id":"fyonv6iENXAK"}},{"cell_type":"code","source":["# DINOv2 (frozen)\n","rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","     backbone=\"dinov2_b14\", feature=\"hyperpigmentation\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=False,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"uwJYmtkuNW0T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### With synth"],"metadata":{"id":"F6JrBjvgNgVC"}},{"cell_type":"code","source":["# DINOv2 (frozen)\n","rep_dino_el, sum_dino_el = repeat_avg_frozen(\n","     backbone=\"dinov2_b14\", feature=\"hyperpigmentation\",\n","     n_runs=3, base_seed=2025,\n","     use_synthetic=True,\n","     C_grid=(0.5,1.0,2.0), do_tta=True\n",")"],"metadata":{"id":"sizcfPMMNWyG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-D7zaoyjNWna"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SF0IbiUkNWim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v8J9tuyYLW8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **FINE TUNING (PANDERM/DINOV2)**"],"metadata":{"id":"WFKkGdJ-LXdo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgPx-HczlMkj"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmneGTzjo4XB"},"outputs":[],"source":["%%bash\n","set -e\n","pip -q install gdown\n","mkdir -p /content/panderm_weights\n","\n","# ViT-Large (paper model)\n","gdown --fuzzy \"https://drive.google.com/file/d/1SwEzaOlFV_gBKf2UzeowMC8z9UH7AQbE/view?usp=sharing\" \\\n","  -O /content/panderm_weights/panderm_ll_data6_checkpoint-499.pth\n","\n","# ViT-Base (smaller)\n","gdown --fuzzy \"https://drive.google.com/file/d/17J4MjsZu3gdBP6xAQi_NMDVvH65a00HB/view?usp=sharing\" \\\n","  -O /content/panderm_weights/panderm_bb_data6_checkpoint-499.pth\n","\n","ls -lh /content/panderm_weights\n"]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedGroupKFold"],"metadata":{"id":"3vQI7sJFuiER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================================================\n","# Backbone toggle (PanDerm / DINOv2)\n","# ================================================================\n","import os\n","import timm\n","from timm.data import resolve_data_config\n","\n","# OPTIONS:\n","#   \"panderm_b16\", \"panderm_l16\"   (needs PANDERM_CKPT)\n","#   \"dinov2_b14\", \"dinov2_l14\"     (pretrained=True via timm)\n","BACKBONE = \"dinov2_b14\"\n","\n","# If you use PanDerm set this path:\n","PANDERM_CKPT = \"/content/panderm_weights/panderm_ll_data6_checkpoint-499.pth\"\n","\n","# Global stats used by the Datasets\n","IMNET_MEAN = (0.485, 0.456, 0.406)\n","IMNET_STD  = (0.229, 0.224, 0.225)\n","\n","def _interpolate_pos_embed(model, posemb):\n","    pe_new = model.pos_embed\n","    if posemb.shape == pe_new.shape:\n","        return posemb\n","    cls_tok, grid = posemb[:, :1], posemb[:, 1:]\n","    cls_tok_new, grid_new = pe_new[:, :1], pe_new[:, 1:]\n","    gs_old = int(grid.shape[1] ** 0.5)\n","    gs_new = int(grid_new.shape[1] ** 0.5)\n","    grid = grid.reshape(1, gs_old, gs_old, -1).permute(0,3,1,2)\n","    grid = torch.nn.functional.interpolate(\n","        grid, size=(gs_new, gs_new), mode=\"bicubic\", align_corners=False\n","    )\n","    grid = grid.permute(0,2,3,1).reshape(1, gs_new*gs_new, -1)\n","    return torch.cat([cls_tok_new, grid], dim=1)\n","\n","def load_panderm_backbone(encoder, ckpt_path, verbose=True):\n","    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n","    sd = ckpt.get(\"model\") or ckpt.get(\"state_dict\") or ckpt\n","\n","    def strip(k):\n","        for p in (\"module.\", \"backbone.\", \"encoder.\", \"model.\"):\n","            if k.startswith(p): return k[len(p):]\n","        return k\n","\n","    enc_sd = encoder.state_dict()\n","    mapped = {}\n","    for k, v in sd.items():\n","        k2 = strip(k)\n","        if k2 == \"pos_embed\" and \"pos_embed\" in enc_sd:\n","            try:\n","                v = _interpolate_pos_embed(encoder, v)\n","            except Exception:\n","                pass\n","        if k2 in enc_sd and enc_sd[k2].shape == v.shape:\n","            mapped[k2] = v\n","\n","    missing = set(enc_sd.keys()) - set(mapped.keys())\n","    encoder.load_state_dict(mapped, strict=False)\n","    if verbose:\n","        print(f\"PanDerm load: matched {len(mapped)}/{len(enc_sd)} params; \"\n","              f\"missing {len(missing)}.\")\n","\n","def _make_timm_name(backbone: str):\n","    if backbone == \"panderm_b16\": return \"vit_base_patch16_224\"\n","    if backbone == \"panderm_l16\": return \"vit_large_patch16_224\"\n","    if backbone == \"dinov2_b14\":  return \"vit_base_patch14_dinov2\"\n","    if backbone == \"dinov2_l14\":  return \"vit_large_patch14_dinov2\"\n","    raise ValueError(f\"Unknown BACKBONE: {backbone}\")\n","\n","def create_encoder_from_choice(backbone=BACKBONE):\n","    # Declare globals BEFORE any use in this function\n","    global IMNET_MEAN, IMNET_STD, CROP_TO, RESIZE_TO\n","\n","    name = _make_timm_name(backbone)\n","\n","    # Create encoder\n","    if backbone.startswith((\"dinov2\")):\n","        enc = timm.create_model(name, pretrained=True, num_classes=0)\n","    else:\n","        enc = timm.create_model(name, pretrained=False, num_classes=0)\n","\n","    # Optional PanDerm checkpoint\n","    if backbone.startswith(\"panderm\"):\n","        assert \"PANDERM_CKPT\" in globals() and os.path.exists(PANDERM_CKPT), \\\n","            \"Set PANDERM_CKPT to a valid file when using a PanDerm backbone.\"\n","        load_panderm_backbone(enc, PANDERM_CKPT, verbose=True)\n","\n","    # Update mean/std & input size for your Datasets\n","    cfg = resolve_data_config({}, model=enc)\n","    mean = tuple(cfg[\"mean\"]) if \"mean\" in cfg else IMNET_MEAN\n","    std  = tuple(cfg[\"std\"])  if \"std\"  in cfg else IMNET_STD\n","    input_size = cfg.get(\"input_size\", (3, 224, 224))\n","    H = input_size[-1] if isinstance(input_size, (list, tuple)) else 224\n","\n","    IMNET_MEAN, IMNET_STD = mean, std\n","    # If CROP_TO/RESIZE_TO don't exist yet, this will create them\n","    CROP_TO  = H\n","    RESIZE_TO = max(384, H * (384 // 224))  # simple heuristic\n","\n","    print(f\"[Backbone={backbone}] input={input_size}  mean={IMNET_MEAN}  std={IMNET_STD}\")\n","    return enc\n"],"metadata":{"id":"9Kcl9zfI8hZm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xsc7bduclQIq"},"outputs":[],"source":["# ================================================================\n","# Setup\n","# ================================================================\n","import os, gc, math, time, random\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","\n","import torch, torch.nn as nn, torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from torchvision import transforms\n","import timm\n","\n","from sklearn.model_selection import GroupShuffleSplit\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import f1_score, classification_report, confusion_matrix\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","SEED   = 42\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)"]},{"cell_type":"code","source":["from torchvision import transforms\n","from torchvision.transforms import functional as TF"],"metadata":{"id":"ogafzgySO4nb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qtz8Wz1zlVGp"},"source":["### Per feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXTzrtS7lUh7"},"outputs":[],"source":["# ================================================================\n","# Config\n","# ================================================================\n","CSV_TRAIN_POOL = \"/content/drive/MyDrive/Skin_project/train_original.csv\"\n","CSV_SYN_WIDE   = \"/content/drive/MyDrive/Skin_project/synthetic_as_wide.csv\"\n","CSV_TEST_FIXED = \"/content/drive/MyDrive/Skin_project/test_original.csv\"\n","\n","# Single feature\n","FEATURE = \"moisture\"\n","\n","# Map feature -> image-path column\n","feature2imgcol = {\n","    \"moisture\": \"moisture_img\",\n","    \"oiliness\": \"oiliness_img\",\n","    \"elasticity\": \"elasticity_img\",\n","    \"texture\": \"texture_img\",\n","    \"hyperpigmentation\": \"hyperpigmentation_img\",\n","}\n","\n","# Label spaces & binary remaps\n","DEFAULT_LABELS = [-1, 0, 1]\n","BINARY_MAP = {\n","    \"oiliness\": { -1: 0, 0: 1, 1: 1 },  # dry vs non-dry\n","    \"moisture\": { -1: 0, 0: 1, 1: 1 },\n","}\n","FEATURE_LABELS = {\n","    \"oiliness\":  [0, 1],\n","    \"moisture\":  [0, 1],\n","    \"texture\":   [-1, 0, 1],\n","    \"elasticity\":[-1, 0, 1],\n","    \"hyperpigmentation\":[-1, 0, 1],\n","}\n","\n","\n","# Image sizes & data params (BiT/ViT style)\n","RESIZE_TO = 384\n","CROP_TO   = 224\n","BATCH     = 8\n","NUM_WORKERS = 2\n","VAL_FRAC_WITHIN_TRAIN = 0.125\n","\n","# TARGETED OVERSAMPLING FOR BINARY\n","# Targeted oversampling for the minority class (oiliness: label 1)\n","TARGET_MINORITY_FRAC = 0.45  # aim for ~35% positive in the training batches\n","AUG_MINOR_PROB = 0.7        # probability to apply targeted photometric aug on minority\n","AUG_MAJOR_PROB = 0.1        # lower prob on majority to avoid shifting distribution\n","\n","# Fine-tuning schedule\n","EPOCHS_WARMUP = 3     # head-only\n","#EPOCHS_FT     = 15   # unfreeze top blocks (or full) at tiny LR\n","LR_HEAD       = 3e-4\n","LR_FT         = 3e-5\n","WD            = 2e-4\n","PATIENCE_F1   = 5    # early stop on val macro-F1\n","\n","# Model size\n","TIMM_BACKBONE = \"vit_large_patch16_224\"  # or \"vit_base_patch16_224\" if RAM is tigh\n","UNFREEZE_LAST_BLOCKS = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CJwDDHBmJrt"},"outputs":[],"source":["# ================================================================\n","# Data loading\n","# ================================================================\n","def load_feature_df_one(csv_path, feature):\n","    df_all  = pd.read_csv(csv_path)\n","    img_col = feature2imgcol[feature]\n","    lbl_col = f\"{feature}_score\"\n","    cols = [\"patient_id\", img_col, lbl_col] + ([\"region\"] if \"region\" in df_all.columns else [])\n","    df  = df_all[cols].copy().rename(columns={img_col:\"image_path\", lbl_col:\"label\"})\n","\n","    df = df[df[\"image_path\"].apply(lambda p: isinstance(p, str) and os.path.exists(p))]\n","    df = df.dropna(subset=[\"label\"])\n","    df[\"label\"] = df[\"label\"].astype(float).round().astype(int)\n","\n","    if feature in BINARY_MAP:\n","        df[\"label\"] = df[\"label\"].map(BINARY_MAP[feature])\n","        df = df.dropna(subset=[\"label\"]).astype({\"label\": int})\n","\n","    allowed = FEATURE_LABELS.get(feature, DEFAULT_LABELS)\n","    df = df[df[\"label\"].isin(allowed)].reset_index(drop=True)\n","    return df\n","\n","def load_feature_df_multi(csv_paths, feature):\n","    frames = [load_feature_df_one(p, feature) for p in csv_paths]\n","    df = pd.concat(frames, ignore_index=True)\n","    return df.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n","\n","def remove_patient_overlap(df_trainpool, df_test):\n","    overlap = set(df_trainpool.patient_id.unique()) & set(df_test.patient_id.unique())\n","    if overlap:\n","        print(f\" Removing {len(overlap)} patient_id overlap(s).\")\n","        df_trainpool = df_trainpool[~df_trainpool.patient_id.isin(overlap)].reset_index(drop=True)\n","    return df_trainpool\n","\n","def make_val_from_trainpool(df_trainpool, val_frac=0.125, seed=42):\n","    \"\"\"\n","    Patient-disjoint AND label-stratified split using StratifiedGroupKFold.\n","    We create several candidate folds and choose the one closest to val_frac.\n","    If the chosen fold ends up single-class (tiny data), we move a few samples\n","    from train to val as a safety net.\n","    \"\"\"\n","    y = df_trainpool[\"label\"].astype(int).values\n","    groups = df_trainpool[\"patient_id\"].values\n","\n","    # number of folds (5 works well; gives ~20% val; we'll pick the closest)\n","    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n","\n","    N = len(df_trainpool)\n","    best = None\n","    for tr_idx, va_idx in sgkf.split(df_trainpool, y=y, groups=groups):\n","        frac = len(va_idx) / N\n","        cand = (abs(frac - val_frac), (tr_idx, va_idx))\n","        if best is None or cand[0] < best[0]:\n","            best = cand\n","\n","    tr_idx, va_idx = best[1]\n","    df_tr = df_trainpool.iloc[tr_idx].reset_index(drop=True)\n","    df_va = df_trainpool.iloc[va_idx].reset_index(drop=True)\n","\n","    # --- Safety net: ensure all classes appear in val\n","    want_labels = set(df_trainpool[\"label\"].unique())\n","    have_labels = set(df_va[\"label\"].unique())\n","    missing = list(want_labels - have_labels)\n","    if missing:\n","        rng = np.random.default_rng(seed)\n","        for cls in missing:\n","            cand = df_tr[df_tr[\"label\"] == cls]\n","            if len(cand) == 0:\n","                continue\n","            take_idx = cand.sample(min(2, len(cand)), random_state=seed).index\n","            df_va = pd.concat([df_va, df_tr.loc[take_idx]], ignore_index=True)\n","            df_tr = df_tr.drop(take_idx).reset_index(drop=True)\n","\n","    return df_tr, df_va\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Txzd-UJmMxH"},"outputs":[],"source":["# ================================================================\n","# Dataset with targeted augmentations for minority class\n","# ================================================================\n","\n","class SkinDataset(Dataset):\n","    def __init__(self, df, lab2idx, training=True, size=None,\n","                 aug_minor_prob=AUG_MINOR_PROB, aug_major_prob=AUG_MAJOR_PROB,\n","                 minority_label_idx=1):\n","        self.paths  = df.image_path.tolist()\n","        self.labels = np.array([lab2idx[int(v)] for v in df.label.values], np.int64)\n","        self.training = training\n","        self.minority_label_idx = minority_label_idx\n","        self.aug_minor_prob = aug_minor_prob\n","        self.aug_major_prob = aug_major_prob\n","        self.size = int(size if size is not None else CROP_TO)\n","\n","        base_train = [\n","            transforms.Resize(RESIZE_TO),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomResizedCrop(self.size, scale=(0.85, 1.0)),\n","        ]\n","        base_eval = [\n","            transforms.Resize(RESIZE_TO),\n","            transforms.CenterCrop(self.size),\n","        ]\n","\n","        self.photometric = transforms.Compose([\n","            transforms.ColorJitter(brightness=0.12, contrast=0.12),\n","            transforms.RandomAdjustSharpness(sharpness_factor=1.4, p=0.5),\n","            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n","        ])\n","\n","        self.to_tensor = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=IMNET_MEAN, std=IMNET_STD)  # <—  backbone stats\n","        ])\n","\n","        self.base_tf = transforms.Compose(base_train if training else base_eval)\n","\n","    def __len__(self): return len(self.paths)\n","\n","    def __getitem__(self, i):\n","        img = Image.open(self.paths[i]).convert(\"RGB\")\n","        y   = self.labels[i]\n","        x   = self.base_tf(img)\n","        if self.training:\n","            p = self.aug_minor_prob if y == self.minority_label_idx else self.aug_major_prob\n","            if random.random() < p:\n","                x = self.photometric(x)\n","        x = self.to_tensor(x)\n","        return x, y\n","\n","\n","class SimpleDataset(Dataset):\n","    def __init__(self, df, lab2idx, training=True, size=None):\n","        self.paths  = df.image_path.tolist()\n","        self.labels = np.array([lab2idx[int(v)] for v in df.label.values], np.int64)\n","        self.training = training\n","        self.size = int(size if size is not None else CROP_TO)\n","\n","        base_train = [\n","            transforms.Resize(RESIZE_TO),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomResizedCrop(self.size, scale=(0.85, 1.0)),\n","        ]\n","        base_eval = [\n","            transforms.Resize(RESIZE_TO),\n","            transforms.CenterCrop(self.size),\n","        ]\n","\n","        self.base_tf = transforms.Compose(base_train if training else base_eval)\n","        self.to_tensor = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=IMNET_MEAN, std=IMNET_STD)  # <— use backbone stats\n","        ])\n","\n","    def __len__(self): return len(self.paths)\n","\n","    def __getitem__(self, i):\n","        img = Image.open(self.paths[i]).convert(\"RGB\")\n","        x = self.base_tf(img)\n","        x = self.to_tensor(x)\n","        y = self.labels[i]\n","        return x, y\n","\n","\n","## ENABLING TTA\n","\n","class TTADataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Produces T augmented views per image for evaluation.\n","    - FiveCrop(224) → 5 crops; with hflip=True → 10 views\n","    - Returns a tensor of shape [T, 3, H, W] and a single label\n","    \"\"\"\n","    def __init__(self, df, lab2idx, hflip=True):\n","        self.paths  = df.image_path.tolist()\n","        self.labels = np.array([lab2idx[int(v)] for v in df.label.values], np.int64)\n","        self.hflip  = hflip\n","        self.pre    = transforms.Resize(RESIZE_TO)\n","        self.crops  = transforms.FiveCrop(CROP_TO)\n","        self.post   = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=IMNET_MEAN, std=IMNET_STD)\n","        ])\n","\n","    def __len__(self): return len(self.paths)\n","\n","    def __getitem__(self, i):\n","        img = Image.open(self.paths[i]).convert(\"RGB\")\n","        img = self.pre(img)\n","        # 5 crops (TL, TR, BL, BR, center)\n","        crops = list(self.crops(img))\n","        if self.hflip:\n","            crops += [TF.hflip(c) for c in crops]\n","        tens = torch.stack([self.post(c) for c in crops], dim=0)  # [T,3,H,W]\n","        y = self.labels[i]\n","        return tens, y\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtKwoItqmSrO"},"outputs":[],"source":["# ================================================================\n","# Cosine head (angular classifier) + residual MLP adapter\n","# ================================================================\n","class CosineClassifier(nn.Module):\n","    def __init__(self, in_dim, n_classes, scale=16.0):\n","        super().__init__()\n","        self.W = nn.Parameter(torch.empty(in_dim, n_classes))\n","        nn.init.xavier_uniform_(self.W)\n","        self.scale = scale\n","    def forward(self, x):\n","        x = F.normalize(x, dim=-1)\n","        W = F.normalize(self.W, dim=0)\n","        return self.scale * (x @ W)\n","\n","class StrongHead(nn.Module):\n","    def __init__(self, in_dim, n_classes, width=1024, depth=2, drop=0.35, scale=16.0):\n","        super().__init__()\n","        self.inp = nn.Sequential(\n","            nn.LayerNorm(in_dim), nn.Linear(in_dim, width), nn.GELU(), nn.Dropout(drop)\n","        )\n","        self.blocks = nn.ModuleList([\n","            nn.Sequential(  # pre-norm residual MLP\n","                nn.LayerNorm(width),\n","                nn.Linear(width, 2*width), nn.GELU(), nn.Dropout(drop),\n","                nn.Linear(2*width, width)\n","            ) for _ in range(depth)\n","        ])\n","        self.cls = CosineClassifier(width, n_classes, scale=scale)\n","    def forward(self, z):\n","        x = self.inp(z)\n","        for b in self.blocks:\n","            x = x + b(x)\n","        return self.cls(x)\n","\n","# ================================================================\n","# PanDerm / ViT backbone loader\n","# ================================================================\n","\n","def create_encoder(panderm_ckpt=None):\n","    # ignored; kept for API compatibility with the rest of your code\n","    return create_encoder_from_choice(BACKBONE)\n","\n","def unfreeze_last_blocks_vit(encoder, n_blocks=UNFREEZE_LAST_BLOCKS):\n","    # For ViT in timm, blocks live in encoder.blocks (nn.Sequential)\n","    for p in encoder.parameters():\n","        p.requires_grad = False\n","    if hasattr(encoder, \"blocks\"):\n","        for b in encoder.blocks[-n_blocks:]:\n","            for p in b.parameters():\n","                p.requires_grad = True\n","        # also unfreeze final norm / head prefeatures if present\n","        for name in [\"norm\", \"pre_logits\"]:\n","            if hasattr(encoder, name):\n","                for p in getattr(encoder, name).parameters():\n","                    p.requires_grad = True\n","    else:\n","        # fallback: unfreeze everything if structure unknown\n","        for p in encoder.parameters():\n","            p.requires_grad = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcPkGhxnmXKd"},"outputs":[],"source":["# ================================================================\n","# Training utilities\n","# ================================================================\n","\n","def build_model(n_classes, panderm_ckpt=None, width=1024, depth=2, drop=0.35, scale=30.0): # width=512, depth=1, drop=0.2, scale=20.0\n","    enc = create_encoder_from_choice(BACKBONE)\n","    head = StrongHead(enc.num_features, n_classes, width=width, depth=depth, drop=drop, scale=scale)\n","    return nn.Sequential(enc, head).to(DEVICE), enc, head\n","\n","def make_weighted_sampler(labels_idx, target_minority_frac=TARGET_MINORITY_FRAC, minority_idx=1):\n","    labels_idx = np.asarray(labels_idx)\n","    c1 = (labels_idx == minority_idx).sum()\n","    c0 = (labels_idx != minority_idx).sum()\n","    if c1 == 0 or c0 == 0:  # degenerate\n","        weights = np.ones_like(labels_idx, dtype=np.float32)\n","        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n","\n","    # weights to achieve expected fraction ~ target_minority_frac\n","    w1 = target_minority_frac / c1\n","    w0 = (1.0 - target_minority_frac) / c0\n","    weights = np.where(labels_idx == minority_idx, w1, w0).astype(np.float32)\n","    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n","\n","def run_epoch(model, loader, optimizer=None, class_weights=None, lbl_smooth=0.05):\n","    train = optimizer is not None\n","    model.train(train)\n","    if class_weights is not None:\n","        ce = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=lbl_smooth)\n","    else:\n","        ce = nn.CrossEntropyLoss(label_smoothing=lbl_smooth)\n","\n","    total_loss, n, correct = 0.0, 0, 0\n","    for batch in loader:\n","        if len(batch) == 3:\n","            x,y,_ = batch\n","        else:\n","            x,y = batch\n","        x = x.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)\n","        logits = model(x)\n","        loss = ce(logits, y)\n","        if train:\n","            optimizer.zero_grad(set_to_none=True)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","        total_loss += loss.item() * x.size(0)\n","        correct    += (logits.argmax(1) == y).sum().item()\n","        n          += x.size(0)\n","    return correct / max(1,n), total_loss / max(1,n)\n","\n","def evaluate_macro_f1(model, loader):\n","    model.eval()\n","    ys, yp = [], []\n","    with torch.no_grad():\n","        for batch in loader:\n","            if len(batch) == 3:\n","                x,y,_ = batch\n","            else:\n","                x,y = batch\n","            x = x.to(DEVICE)\n","            logits = model(x)\n","            yp.append(logits.argmax(1).cpu().numpy()); ys.append(y.numpy())\n","    y_true = np.concatenate(ys); y_pred = np.concatenate(yp)\n","    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n","    return f1, y_true, y_pred\n","\n","\n","\n","# TTA\n","\n","def make_tta_loader(df, lab2idx, hflip=True, batch=BATCH//2):\n","    \"\"\"\n","    Builds a DataLoader for TTA. Batch is smaller (each sample expands to T views).\n","    \"\"\"\n","    ds = TTADataset(df, lab2idx, hflip=hflip)\n","    return DataLoader(ds, batch_size=batch, shuffle=False,\n","                      num_workers=NUM_WORKERS, pin_memory=True)\n","\n","def eval_binary_with_tta(model, loader, threshold=0.5, device=DEVICE,\n","                         temperature=1.0, return_scores=False):\n","    \"\"\"\n","    Average logits across TTA views, apply temperature, softmax → P1, then threshold.\n","    If return_scores=True, also return the probability scores used for thresholding.\n","    \"\"\"\n","    model.eval()\n","    ys, yp, scores = [], [], []\n","    with torch.no_grad():\n","        for xT, y in loader:\n","            B, T, C, H, W = xT.shape\n","            xT = xT.view(B*T, C, H, W).to(device, non_blocking=True)\n","            logits = model(xT).view(B, T, -1).mean(1) / float(temperature)  # [B,C]\n","            p1 = logits.softmax(1)[:, 1].cpu().numpy()\n","            pred = (p1 >= threshold).astype(int)\n","            ys.append(y.numpy()); yp.append(pred); scores.append(p1)\n","    y_true = np.concatenate(ys); y_pred = np.concatenate(yp)\n","    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n","    if return_scores:\n","        return f1, y_true, y_pred, np.concatenate(scores)\n","    return f1, y_true, y_pred\n","\n","def eval_multiclass_with_tta(model, loader, device=DEVICE,\n","                             temperature=1.0, return_proba=False):\n","    \"\"\"\n","    Average logits across TTA views, apply temperature, softmax → probs, argmax.\n","    If return_proba=True, also return the probability matrix [N,C].\n","    \"\"\"\n","    model.eval()\n","    ys, yp, Ps = [], [], []\n","    with torch.no_grad():\n","        for xT, y in loader:\n","            B, T, C, H, W = xT.shape\n","            xT = xT.view(B*T, C, H, W).to(device, non_blocking=True)\n","            logits = model(xT).view(B, T, -1).mean(1) / float(temperature)  # [B,C]\n","            prob = logits.softmax(1).cpu().numpy()\n","            pred = prob.argmax(1)\n","            ys.append(y.numpy()); yp.append(pred); Ps.append(prob)\n","    y_true = np.concatenate(ys); y_pred = np.concatenate(yp)\n","    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n","    if return_proba:\n","        return f1, y_true, y_pred, np.vstack(Ps)\n","    return f1, y_true, y_pred\n","\n","def tune_threshold_binary_tta(model, loader, device=DEVICE, temperature=1.0):\n","    \"\"\"\n","    Sweep threshold over TTA-averaged, temperature-scaled probabilities to maximize macro-F1.\n","    \"\"\"\n","    model.eval()\n","    scores, ys = [], []\n","    with torch.no_grad():\n","        for xT, y in loader:\n","            B, T, C, H, W = xT.shape\n","            xT = xT.view(B*T, C, H, W).to(device, non_blocking=True)\n","            logits = model(xT).view(B, T, -1).mean(1) / float(temperature)\n","            p1 = logits.softmax(1)[:, 1].cpu().numpy()\n","            scores.append(p1); ys.append(y.numpy())\n","    y = np.concatenate(ys); p = np.concatenate(scores)\n","    best_t, best_f1 = 0.5, -1.0\n","    for t in np.linspace(0.2, 0.8, 31):\n","        pred = (p >= t).astype(int)\n","        f1 = f1_score(y, pred, average=\"macro\", zero_division=0)\n","        if f1 > best_f1:\n","            best_f1, best_t = f1, t\n","    return best_t\n"]},{"cell_type":"code","source":["from sklearn.metrics import (\n","    f1_score, classification_report, confusion_matrix,\n","    accuracy_score, matthews_corrcoef, cohen_kappa_score,\n","    top_k_accuracy_score, roc_auc_score\n",")"],"metadata":{"id":"V4CJJOdRKwMN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- PR helpers (TTA-aware, Temperature-aware) ---\n","from sklearn.metrics import precision_recall_curve, average_precision_score\n","from sklearn.preprocessing import label_binarize\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","\n","def tta_binary_scores(model, loader, temperature: float = 1.0, device=DEVICE):\n","    \"\"\"\n","    Returns: y (0/1), score for positive class after TTA averaging and temp scaling.\n","    \"\"\"\n","    model.eval(); ys, scores = [], []\n","    T = float(temperature)\n","    with torch.no_grad():\n","        for xT, y in loader:                      # xT: [B,T,3,H,W]\n","            B,Tv,C,H,W = xT.shape\n","            xT = xT.view(B*Tv, C, H, W).to(device, non_blocking=True)\n","            logits = model(xT) / T\n","            p1 = logits.softmax(1)[:, 1]          # [B*T]\n","            p1 = p1.view(B, Tv).mean(1).cpu().numpy()   # TTA average\n","            ys.append(y.numpy()); scores.append(p1)\n","    return np.concatenate(ys), np.concatenate(scores)\n","\n","def tta_multiclass_probs(model, loader, temperature: float = 1.0, device=DEVICE):\n","    \"\"\"\n","    Returns: y (class indices 0..C-1), and probs [N,C] after TTA avg + temp scaling.\n","    \"\"\"\n","    model.eval(); ys, probs = [], []\n","    T = float(temperature)\n","    with torch.no_grad():\n","        for xT, y in loader:\n","            B,Tv,C,H,W = xT.shape\n","            xT = xT.view(B*Tv, C, H, W).to(device, non_blocking=True)\n","            logits = model(xT) / T\n","            p = logits.softmax(1)                 # [B*T,C]\n","            p = p.view(B, Tv, -1).mean(1).cpu().numpy()  # [B,C]\n","            ys.append(y.numpy()); probs.append(p)\n","    return np.concatenate(ys), np.vstack(probs)\n","\n","def plot_pr_binary_tta(model, loader, temperature: float = 1.0,\n","                       title='PR (binary)', show_temperature: bool = False):\n","    y, s = tta_binary_scores(model, loader, temperature=temperature)\n","    prec, rec, _ = precision_recall_curve(y, s)\n","    ap = average_precision_score(y, s)\n","\n","    # optional temperature tag without a separate var\n","    ttag = f\" (T={temperature:.2f})\" if (show_temperature and temperature != 1.0) else \"\"\n","    plt.figure(figsize=(5,4))\n","    plt.step(rec, prec, where='post')\n","    plt.xlabel('Recall'); plt.ylabel('Precision')\n","    plt.title(f'{title}{ttag} | AP={ap:.3f}')\n","    plt.grid(True); plt.tight_layout(); plt.show()\n","\n","\n","def plot_pr_multiclass_tta(model, loader, n_classes, class_names=None,\n","                           temperature: float = 1.0,\n","                           title='PR (multiclass)', show_temperature: bool = False,\n","                           show_micro: bool = True):\n","    y, P = tta_multiclass_probs(model, loader, temperature=temperature)\n","    Y = label_binarize(y, classes=np.arange(n_classes))\n","\n","    # micro-average\n","    prec_mi, rec_mi, _ = precision_recall_curve(Y.ravel(), P.ravel())\n","    ap_mi = average_precision_score(Y, P, average='micro')\n","\n","    ttag = f\" (T={temperature:.2f})\" if (show_temperature and temperature != 1.0) else \"\"\n","    plt.figure(figsize=(6,5))\n","    if show_micro:\n","        plt.step(rec_mi, prec_mi, where='post', label=f'micro (AP={ap_mi:.3f})')\n","\n","    # per-class curves\n","    for c in range(n_classes):\n","        prec, rec, _ = precision_recall_curve(Y[:, c], P[:, c])\n","        ap = average_precision_score(Y[:, c], P[:, c])\n","        name = class_names[c] if class_names else f'class {c}'\n","        plt.step(rec, prec, where='post', alpha=0.8, label=f'{name} (AP={ap:.3f})')\n","\n","    plt.xlabel('Recall'); plt.ylabel('Precision')\n","    plt.title(f'{title}{ttag}')\n","    plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n","\n"],"metadata":{"id":"fTKEXTQsHrGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# Temperature scaling (with TTA)\n","# ===============================\n","import torch.nn.functional as F\n","\n","def _collect_logits_tta(model, loader, device=DEVICE):\n","    \"\"\"\n","    Returns:\n","      logits  : [N, C] mean logits per image (TTA averaged, no softmax)\n","      targets : [N]\n","    \"\"\"\n","    model.eval()\n","    logits_all, ys = [], []\n","    with torch.no_grad():\n","        for xT, y in loader:\n","            B, T, C, H, W = xT.shape\n","            xT = xT.view(B*T, C, H, W).to(device, non_blocking=True)\n","            logits = model(xT).view(B, T, -1).mean(1)   # average logits across TTA views\n","            logits_all.append(logits.cpu())\n","            ys.append(y.cpu())\n","    return torch.cat(logits_all, 0), torch.cat(ys, 0).long()\n","\n","def fit_temperature(model, va_loader_tta, init_T=1.0, max_iter=50):\n","    \"\"\"\n","    Fit a single temperature T on the validation set by minimizing NLL (cross-entropy)\n","    over the TTA-averaged logits (recommended practice).\n","    \"\"\"\n","    # Collect TTA-mean logits on CPU for stable LBFGS\n","    logits, y = _collect_logits_tta(model, va_loader_tta)\n","\n","    # Optimize log_T to keep T positive\n","    log_T = torch.nn.Parameter(torch.log(torch.tensor([init_T], dtype=torch.float32)))\n","    opt   = torch.optim.LBFGS([log_T], lr=0.5, max_iter=max_iter, line_search_fn=\"strong_wolfe\")\n","    ce    = torch.nn.CrossEntropyLoss()\n","\n","    def closure():\n","        opt.zero_grad()\n","        T = torch.exp(log_T)\n","        loss = ce(logits / T, y)\n","        loss.backward()\n","        return loss\n","\n","    opt.step(closure)\n","    T = float(torch.exp(log_T).clamp(0.5, 5.0).item())   # reasonable clamp\n","    return T\n"],"metadata":{"id":"UBI_GvTEFiee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_one_feature(feature=FEATURE, use_synthetic=False, seed=42):\n","    # 1) sync data stats & sizes to the chosen backbone\n","    _ = create_encoder_from_choice(BACKBONE)  # sets IMNET_MEAN/STD and CROP_TO/RESIZE_TO\n","    del _\n","\n","    # ---- build dataframes\n","    pools   = [CSV_TRAIN_POOL] + ([CSV_SYN_WIDE] if use_synthetic else [])\n","    df_pool = load_feature_df_multi(pools, feature)\n","    df_test = load_feature_df_one(CSV_TEST_FIXED, feature)\n","    df_pool = remove_patient_overlap(df_pool, df_test)\n","    df_tr, df_va = make_val_from_trainpool(df_pool, val_frac=VAL_FRAC_WITHIN_TRAIN, seed=seed)\n","\n","    print(\"y_tr:\", df_tr.label.value_counts().sort_index().to_dict(),\n","      \"| y_va:\", df_va.label.value_counts().sort_index().to_dict(),\n","      \"| y_te:\", df_test.label.value_counts().sort_index().to_dict())\n","\n","\n","    # ---- label indexing\n","    classes = np.array(sorted(FEATURE_LABELS.get(feature, DEFAULT_LABELS)))\n","    lab2idx = {int(l): i for i, l in enumerate(classes)}\n","    nC      = len(classes)\n","    is_binary = (nC == 2)\n","\n","    lbl_smooth = 0.0 if nC == 3 else 0.05\n","\n","    # ---- datasets / loaders\n","    if is_binary and feature in [\"oiliness\", \"moisture\"]:\n","        # keep your targeted dataset & sampler for binary\n","        minority_idx = 1  # by your mapping\n","        # binary (oiliness/moisture)\n","        tr_ds = SkinDataset(df_tr, lab2idx, training=True,  size=CROP_TO, minority_label_idx=minority_idx)\n","        va_ds = SkinDataset(df_va, lab2idx, training=False, size=CROP_TO,minority_label_idx=minority_idx)\n","        te_ds = SkinDataset(df_test,lab2idx, training=False, size=CROP_TO, minority_label_idx=minority_idx)\n","\n","        y_tr_idx = np.array([lab2idx[int(v)] for v in df_tr.label.values], np.int64)\n","        sampler  = make_weighted_sampler(y_tr_idx, TARGET_MINORITY_FRAC, minority_idx)\n","\n","        tr_loader = DataLoader(tr_ds, batch_size=BATCH, sampler=sampler,\n","                               num_workers=NUM_WORKERS, pin_memory=True)\n","        va_loader = DataLoader(va_ds, batch_size=BATCH, shuffle=False,\n","                               num_workers=NUM_WORKERS, pin_memory=True)\n","        te_loader = DataLoader(te_ds, batch_size=BATCH, shuffle=False,\n","                               num_workers=NUM_WORKERS, pin_memory=True)\n","\n","        cw_t = None  # keeping it off with sampler\n","    else:\n","        # 3-class tasks: SimpleDataset, no targeted oversampling\n","        #(hyperpigmentation / elasticity / texture)\n","        tr_ds = SimpleDataset(df_tr, lab2idx, training=True,  size=CROP_TO)\n","        va_ds = SimpleDataset(df_va, lab2idx, training=False, size=CROP_TO)\n","        te_ds = SimpleDataset(df_test,lab2idx, training=False, size=CROP_TO)\n","\n","\n","        tr_loader = DataLoader(tr_ds, batch_size=BATCH, shuffle=True,\n","                               num_workers=NUM_WORKERS, pin_memory=True)\n","        va_loader = DataLoader(va_ds, batch_size=BATCH, shuffle=False,\n","                               num_workers=NUM_WORKERS, pin_memory=True)\n","        te_loader = DataLoader(te_ds, batch_size=BATCH, shuffle=False,\n","                               num_workers=NUM_WORKERS, pin_memory=True)\n","\n","        # mild balancing can help if 3-class is skewed\n","        y_tr_idx = np.array([lab2idx[int(v)] for v in df_tr.label.values], np.int64)\n","        cw       = compute_class_weight(\"balanced\", classes=np.arange(nC), y=y_tr_idx)\n","        cw_t     = torch.tensor(cw, dtype=torch.float32, device=DEVICE)\n","\n","    # ---- TTA loaders for val/test (works for both binary & 3-class)\n","    va_loader_tta = make_tta_loader(df_va,   lab2idx, hflip=True)   # 10 views\n","    te_loader_tta = make_tta_loader(df_test, lab2idx, hflip=True)\n","\n","    # ---- build model\n","    model, encoder, head = build_model(nC, PANDERM_CKPT)\n","\n","    # ============================================================\n","    # Warm-up (encoder frozen) — train head only\n","    # ============================================================\n","    for p in encoder.parameters():\n","        p.requires_grad = False\n","    opt_head = torch.optim.AdamW(head.parameters(), lr=LR_HEAD, weight_decay=WD)\n","\n","    print(\"\\n=== Warm-up (frozen encoder) ===\")\n","    best_f1, best_state = -1.0, None\n","    for ep in range(EPOCHS_WARMUP):\n","        tr_acc, tr_loss = run_epoch(\n","            model, tr_loader,\n","            optimizer=opt_head,\n","            class_weights=cw_t,\n","            lbl_smooth=lbl_smooth,\n","        )\n","\n","        # ---- TTA-based validation metric ----\n","        if is_binary:\n","            val_f1, _, _ = eval_binary_with_tta(model, va_loader_tta, threshold=0.5)\n","        else:\n","            val_f1, _, _ = eval_multiclass_with_tta(model, va_loader_tta)\n","\n","        print(f\"Warm {ep+1}/{EPOCHS_WARMUP} — tr_acc={tr_acc:.3f}  val_macroF1[TTA]={val_f1:.3f}\")\n","\n","        if val_f1 > best_f1:\n","            best_f1 = val_f1\n","            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n","\n","    if best_state:\n","        model.load_state_dict(best_state)\n","    torch.cuda.empty_cache() if DEVICE == \"cuda\" else None\n","    gc.collect()\n","\n","    # ============================================================\n","    # Fine-tune — unfreeze last ViT blocks with tiny LR\n","    # ============================================================\n","    print(\"\\n=== Fine-tune (tiny LR, unfreeze last blocks) ===\")\n","    unfreeze_last_blocks_vit(encoder, n_blocks=UNFREEZE_LAST_BLOCKS)\n","    params_ft = [p for p in model.parameters() if p.requires_grad]\n","    opt_ft    = torch.optim.AdamW(params_ft, lr=LR_FT, weight_decay=WD)\n","\n","    best_f1, best_state, stale = -1.0, None, 0\n","    for ep in range(EPOCHS_FT):\n","        tr_acc, tr_loss = run_epoch(\n","            model, tr_loader,\n","            optimizer=opt_ft,\n","            class_weights=cw_t,\n","            lbl_smooth=lbl_smooth,\n","        )\n","\n","        # ---- TTA-based validation metric ----\n","        if is_binary:\n","            val_f1, _, _ = eval_binary_with_tta(model, va_loader_tta, threshold=0.5)\n","        else:\n","            val_f1, _, _ = eval_multiclass_with_tta(model, va_loader_tta)\n","\n","        print(f\"FT {ep+1}/{EPOCHS_FT} — tr_acc={tr_acc:.3f}  val_macroF1[TTA]={val_f1:.3f}\")\n","\n","        if val_f1 > best_f1 + 1e-6:\n","            best_f1 = val_f1\n","            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n","            stale = 0\n","        else:\n","            stale += 1\n","            if stale >= PATIENCE_F1:\n","                print(\"Early stopping on val macro-F1 (TTA).\")\n","                break\n","\n","    if best_state:\n","        model.load_state_dict(best_state)\n","\n","\n","\n","\n","    # ============================================================\n","    # TTA evaluation\n","    # ============================================================\n","\n","    # ---- Temperature scaling on validation (with TTA) ----\n","    T = fit_temperature(model, va_loader_tta)\n","    print(f\"Fitted temperature (val/TTA): T = {T:.3f}\")\n","\n","    # ---- TTA evaluation + richer metrics (with TS) ----\n","    if is_binary:\n","        # tune threshold on val using temperature-scaled probs\n","        best_t = tune_threshold_binary_tta(model, va_loader_tta, temperature=T)\n","        print(f\"Tuned threshold (TTA+TS) on val: {best_t:.3f}\")\n","\n","        # test with the same T and threshold; also return scores for AUROC/PR\n","        test_f1, y_true, y_pred, s = eval_binary_with_tta(\n","            model, te_loader_tta, threshold=best_t, temperature=T, return_scores=True\n","        )\n","\n","        acc   = accuracy_score(y_true, y_pred)\n","        f1_w  = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n","        mcc   = matthews_corrcoef(y_true, y_pred)\n","        qwk   = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n","        auc   = roc_auc_score(y_true, s)  # binary AUROC\n","\n","        print(f\"\\n[{feature}] Test (TTA+TS) — macro-F1={test_f1:.3f} | \"\n","              f\"weighted-F1={f1_w:.3f} | acc={acc:.3f} | MCC={mcc:.3f} | \"\n","              f\"QWK={qwk:.3f} | AUROC={auc:.3f}\")\n","\n","    else:\n","        # test with temperature; also return P for AUROC/top-k and PR curves\n","        test_f1, y_true, y_pred, P = eval_multiclass_with_tta(\n","            model, te_loader_tta, temperature=T, return_proba=True\n","        )\n","\n","        acc       = accuracy_score(y_true, y_pred)                 # = micro-F1 (multiclass)\n","        f1_w      = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n","        mcc       = matthews_corrcoef(y_true, y_pred)\n","        qwk       = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n","        top2      = top_k_accuracy_score(y_true, P, k=2, labels=list(range(P.shape[1])))\n","        auc_micro = roc_auc_score(y_true, P, multi_class=\"ovr\", average=\"micro\")\n","        auc_macro = roc_auc_score(y_true, P, multi_class=\"ovr\", average=\"macro\")\n","\n","        print(f\"\\n[{feature}] Test (TTA+TS) — macro-F1={test_f1:.3f} | \"\n","              f\"weighted-F1={f1_w:.3f} | acc={acc:.3f} | MCC={mcc:.3f} | \"\n","              f\"QWK={qwk:.3f} | Top-2={top2:.3f} | \"\n","              f\"AUROC(micro/macro)={auc_micro:.3f}/{auc_macro:.3f}\")\n","\n","\n","    # keep your detailed report\n","    print(classification_report(y_true, y_pred, digits=3))\n","    print(\"Confusion (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n","\n","    # PR curves using the same temperature\n","    if is_binary:\n","        plot_pr_binary_tta(model, te_loader_tta, temperature=T, title='moisture · PR', show_temperature=True)\n","        #plot_pr_binary_tta(model, te_loader_tta, temperature=T, title=f'{feature} · PR Curve')\n","        # for saving: auroc already computed as 'auc'\n","        auroc_micro = None; auroc_macro = None; top2 = None\n","    else:\n","        class_names = [str(l) for l in classes]\n","        plot_pr_multiclass_tta(model, te_loader_tta, n_classes=nC, class_names=['-1','0','1'],\n","                       temperature=T, title='texture · PR', show_temperature=True)\n","        #plot_pr_multiclass_tta(model, te_loader_tta, n_classes=nC,\n","                               #class_names=class_names, temperature=T,\n","                               #title=f'{feature} · PR Curve')\n","        # 'auc_micro' and 'auc_macro' are already computed above as auc_micro/macro\n","        auc = None  # binary-only value\n","\n","    # ---------- SAVE RESULT ROW ----------\n","   # result_row = {\n","       # \"backbone\": BACKBONE,\n","        #\"feature\": feature,\n","       # \"use_synthetic\": bool(use_synthetic),\n","       # \"seed\": int(seed),\n","       # \"temperature\": float(T),\n","       # \"threshold\": float(best_t) if is_binary else None,\n","       # \"macro_f1\": float(test_f1),\n","      #  \"weighted_f1\": float(f1_w),\n","      #  \"accuracy\": float(acc),\n","      #  \"mcc\": float(mcc),\n","      #  \"qwk\": float(qwk),\n","      #  \"auroc\": float(auc) if is_binary else None,\n","      #  \"top2\": float(top2) if not is_binary else None,\n","     #   \"auroc_micro\": float(auc_micro) if not is_binary else None,\n","      #  \"auroc_macro\": float(auc_macro) if not is_binary else None,\n","      #  \"n_classes\": int(nC),\n","      #  \"n_train\": int(len(df_tr)),\n","      ##  \"n_val\": int(len(df_va)),\n","      #  \"n_test\": int(len(df_test)),\n","   # }\n","   # _append_result(result_row)\n","\n","    return model, float(test_f1)#, result_row\n"],"metadata":{"id":"PlHU0bhw-1F1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"B6R7cnizvInW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# UMAP"],"metadata":{"id":"4UUmOVIM7OO9"}},{"cell_type":"code","source":["# If needed:\n","!pip install umap-learn"],"metadata":{"id":"Ec1ApeaA2LkT","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===== UMAP on PanDerm embeddings =====\n","try:\n","    import umap\n","except Exception:\n","    umap = None\n","\n","import torch, numpy as np, pandas as pd\n","from PIL import Image\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","\n","RANDOM_SEED = 42\n","\n","# --- eval-time preprocessing (match your eval pipeline) ---\n","RESIZE_TO, CROP_TO = 384, 224\n","_panderm_eval_tf = transforms.Compose([\n","    transforms.Resize(RESIZE_TO),\n","    transforms.CenterCrop(CROP_TO),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n","])\n","\n","class _PathsDS(torch.utils.data.Dataset):\n","    def __init__(self, paths):\n","        self.paths = list(paths)\n","    def __len__(self): return len(self.paths)\n","    def __getitem__(self, i):\n","        p = self.paths[i]\n","        im = Image.open(p).convert(\"RGB\")\n","        x  = _panderm_eval_tf(im)\n","        return x, i\n","\n","@torch.no_grad()\n","def _encode_with_panderm(paths, encoder, batch=64, device=None, desc=\"PanDerm UMAP\"):\n","    \"\"\"Return embeddings [N, D] for a list of image paths using the ViT encoder (num_classes=0).\"\"\"\n","    encoder.eval()\n","    dev = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    encoder.to(dev)\n","\n","    ds = _PathsDS(paths)\n","    dl = torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False,\n","                                     num_workers=2, pin_memory=True)\n","    embs = [None] * len(paths)\n","    for xb, idx in dl:\n","        xb = xb.to(dev, non_blocking=True)\n","        feats = encoder(xb)                 # timm ViT with num_classes=0 -> [B, D]\n","        feats = feats.detach().cpu().numpy()\n","        for f, j in zip(feats, idx.numpy()):\n","            embs[j] = f\n","    return np.stack(embs, axis=0)\n","\n","def _pick_encoder_from_model(model):\n","    \"\"\"If you pass the full (encoder+head) nn.Sequential, return the encoder part.\"\"\"\n","    import torch.nn as nn\n","    if isinstance(model, nn.Sequential):\n","        # assume [encoder, head]\n","        return model[0]\n","    return model  # already an encoder\n","\n","def plot_umap_feature_panderm(\n","    feature, train_csvs, encoder_or_model,\n","    include_test=False, n_neighbors=15, min_dist=0.1, metric=\"cosine\",\n","    per_class_max=None, title_suffix=\"PanDerm\"\n","):\n","    \"\"\"\n","    UMAP on *PanDerm* embeddings for a single feature. Colors = class labels.\n","    - encoder_or_model: pass the encoder (timm ViT with num_classes=0) or your trained model;\n","      if it's your trained nn.Sequential(encoder, head), we'll use just the encoder.\n","    - per_class_max: cap per-class points to keep plots balanced/legible.\n","    \"\"\"\n","    if umap is None:\n","        print(\"UMAP not installed. Run: pip install umap-learn\")\n","        return\n","\n","    enc = _pick_encoder_from_model(encoder_or_model)\n","\n","    # 1) Collect rows (uses your existing helpers & binary remaps)\n","    frames = [load_feature_df_one(p, feature) for p in train_csvs]\n","    df = pd.concat(frames, ignore_index=True)\n","    if include_test:\n","        df_te = load_feature_df_one(CSV_TEST_FIXED, feature)\n","        df_te = df_te.assign(_split=\"test\")\n","        df = pd.concat([df.assign(_split=\"train\"), df_te], ignore_index=True)\n","    else:\n","        df = df.assign(_split=\"train\")\n","\n","    # Ensure paths exist\n","    df = df[df[\"image_path\"].apply(lambda p: isinstance(p, str) and os.path.exists(p))].reset_index(drop=True)\n","    if len(df) == 0:\n","        print(\"No images found for UMAP.\"); return\n","\n","    # 2) Compute PanDerm embeddings (eval transforms, no aug)\n","    X = _encode_with_panderm(df.image_path.values, enc, batch=64)\n","\n","    # 3) Labels (already remapped by load_feature_df_one)\n","    y = df.label.values.astype(int)\n","\n","    # 4) Optional balanced subsample per class (helps heavy imbalance)\n","    if per_class_max is not None:\n","        keep = []\n","        rng = np.random.RandomState(0)\n","        for cls in np.unique(y):\n","            idx = np.where(y == cls)[0]\n","            if len(idx) > per_class_max:\n","                idx = rng.choice(idx, per_class_max, replace=False)\n","            keep.extend(idx.tolist())\n","        keep = np.array(sorted(keep))\n","        X, y, df = X[keep], y[keep], df.iloc[keep].reset_index(drop=True)\n","\n","    # 5) UMAP\n","    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist,\n","                        metric=metric, random_state=RANDOM_SEED)\n","    X2 = reducer.fit_transform(X)\n","\n","    # 6) Plot\n","    plt.figure(figsize=(6.4, 5.4))\n","    for cls in np.unique(y):\n","        m = (y == cls)\n","        plt.scatter(X2[m, 0], X2[m, 1], s=22, alpha=0.85, label=f\"class {cls}\")\n","    plt.title(f\"{feature} · UMAP · {title_suffix}\")\n","    plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n","    plt.legend(); plt.grid(True); plt.show()\n"],"metadata":{"id":"mjEfMrmP7DMf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# Summary CSV (mean/std only)\n","# ===============================\n","RESULTS_SUMMARY_CSV = \"/content/drive/MyDrive/Skin_project/results_summary.csv\"\n","\n","def _init_summary_csv(path=RESULTS_SUMMARY_CSV):\n","    cols = [\n","        \"backbone\", \"feature\", \"use_synthetic\", \"n_runs\",\n","        \"mean_macro_f1\", \"std_macro_f1\", \"min_macro_f1\", \"max_macro_f1\"\n","    ]\n","    if not os.path.exists(path):\n","        pd.DataFrame(columns=cols).to_csv(path, index=False)\n","\n","def _append_summary(backbone, feature, use_synthetic, n_runs, f1s, path=RESULTS_SUMMARY_CSV):\n","    row = {\n","        \"backbone\": str(backbone),\n","        \"feature\": str(feature),\n","        \"use_synthetic\": bool(use_synthetic),\n","        \"n_runs\": int(n_runs),\n","        \"mean_macro_f1\": float(np.mean(f1s)),\n","        \"std_macro_f1\": float(np.std(f1s, ddof=1)) if len(f1s) > 1 else 0.0,\n","        \"min_macro_f1\": float(np.min(f1s)),\n","        \"max_macro_f1\": float(np.max(f1s)),\n","    }\n","    header = not os.path.exists(path)\n","    pd.DataFrame([row]).to_csv(path, mode=\"a\", header=header, index=False)\n","\n","# ===============================\n","# Averaging wrapper\n","# ===============================\n","def repeat_avg(feature, n_runs=3, base_seed=2025, use_synthetic=False):\n","    \"\"\"\n","    Runs train_one_feature n_runs times and saves ONLY the summary stats\n","    (mean ± std, min, max) per (backbone, feature, use_synthetic).\n","    \"\"\"\n","    global BACKBONE  # uses your existing global backbone choice\n","\n","    f1s = []\n","    rows = []\n","    for i in range(n_runs):\n","        s = base_seed + i\n","        random.seed(s); np.random.seed(s)\n","        torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n","        # assumes train_one_feature returns (model, f1, row_dict)\n","        _, f1, row = train_one_feature(feature=feature, use_synthetic=use_synthetic, seed=s)\n","        f1s.append(float(f1)); rows.append(row)\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","\n","    f1s = np.array(f1s, dtype=float)\n","\n","    summary = (\n","        f\"{feature} | backbone={BACKBONE} | use_synthetic={use_synthetic} | \"\n","        f\"{n_runs} runs → mean={f1s.mean():.3f} ± {f1s.std(ddof=1) if len(f1s)>1 else 0.0:.3f} \"\n","        f\"(min={f1s.min():.3f}, max={f1s.max():.3f})\"\n","    )\n","    print(summary)\n","\n","    # ensure CSV exists, then append one summary row\n","    _init_summary_csv()\n","    _append_summary(BACKBONE, feature, use_synthetic, n_runs, f1s)\n","\n","    return f1s, rows\n"],"metadata":{"id":"IsJxaFki2iSy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DINOv2 (Base)"],"metadata":{"id":"5W7jhjjCyg6v"}},{"cell_type":"code","source":["# 3 RUNS WITH AVERAGE\n","import random, numpy as np, torch\n","\n","def repeat_avg(feature, n_runs=3, base_seed=2025, use_synthetic=False):\n","    f1s = []\n","    rows = []\n","    for i in range(n_runs):\n","        s = base_seed + i\n","        random.seed(s); np.random.seed(s)\n","        torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n","        _, f1, row = train_one_feature(feature=feature, use_synthetic=use_synthetic, seed=s)\n","        f1s.append(float(f1))#; rows.append(row)\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","    f1s = np.array(f1s, float)\n","    print(f\"{feature} | {n_runs} runs → mean={f1s.mean():.3f} ± {f1s.std():.3f} \"\n","          f\"(min={f1s.min():.3f}, max={f1s.max():.3f})\")\n","    #print(f\"  - run {i+1}/{n_runs} (seed={s})\", flush=True)\n","    return f1s #, rows"],"metadata":{"id":"bKMZ7gBQROUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    #_init_results_csv()\n","    # single-feature\n","    repeat_avg(feature=\"texture\", n_runs=3, base_seed=42, use_synthetic=False)\n","    repeat_avg(feature=\"texture\", n_runs=3, base_seed=42, use_synthetic=True)"],"metadata":{"id":"I8QFiKxUQ3ew"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    _init_results_csv()\n","    # single-feature\n","    #repeat_avg(feature=\"moisture\", n_runs=3, base_seed=2025, use_synthetic=False)\n","    #repeat_avg(feature=\"moisture\", n_runs=3, base_seed=2025, use_synthetic=True)\n","\n","    # or all features:\n","    df_summary = run_all_features()"],"metadata":{"collapsed":true,"id":"fa00KjCHybRR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PANDERM (Large)"],"metadata":{"id":"mT5HArqoOCtN"}},{"cell_type":"code","source":["# 3 RUNS WITH AVERAGE\n","import random, numpy as np, torch\n","\n","def repeat_avg(feature, n_runs=3, base_seed=2025, use_synthetic=False):\n","    f1s = []\n","    rows = []\n","    for i in range(n_runs):\n","        s = base_seed + i\n","        random.seed(s); np.random.seed(s)\n","        torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n","        _, f1, row = train_one_feature(feature=feature, use_synthetic=use_synthetic, seed=s)\n","        f1s.append(float(f1)); rows.append(row)\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","    f1s = np.array(f1s, float)\n","    print(f\"{feature} | {n_runs} runs → mean={f1s.mean():.3f} ± {f1s.std():.3f} \"\n","          f\"(min={f1s.min():.3f}, max={f1s.max():.3f})\")\n","    #print(f\"  - run {i+1}/{n_runs} (seed={s})\", flush=True)\n","    return f1s, rows"],"metadata":{"id":"uc7Vx0S6SQvF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ALL FEATURES"],"metadata":{"id":"yFCFwqfcVJmL"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n"],"metadata":{"id":"xVGiQ1jWWdUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_all_features(features=None, n_runs=3, base_seed=2025):\n","    _init_results_csv()\n","    if features is None:\n","        features = list(FEATURE_LABELS.keys())\n","    summary = []\n","    for use_syn in (False, True):\n","        print(f\"\\n==== use_synthetic={use_syn} ====\")\n","        for feat in features:\n","            print(f\"→ Training feature: {feat}  | runs={n_runs}  | base_seed={base_seed}\", flush=True)\n","            f1s, rows = repeat_avg(feature=feat, n_runs=n_runs, base_seed=base_seed, use_synthetic=use_syn)\n","            summary.append({\n","                \"feature\": feat,\n","                \"use_synthetic\": use_syn,\n","                \"mean_macro_f1\": float(f1s.mean()),\n","                \"std_macro_f1\": float(f1s.std())\n","            })\n","    return pd.DataFrame(summary)\n"],"metadata":{"id":"sWPEMFhmSfuk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    _init_results_csv()\n","    # single-feature\n","    #repeat_avg(feature=\"moisture\", n_runs=3, base_seed=2025, use_synthetic=False)\n","    #repeat_avg(feature=\"moisture\", n_runs=3, base_seed=2025, use_synthetic=True)\n","\n","    # or all features:\n","    df_summary = run_all_features()"],"metadata":{"id":"--ayCF6TSfsj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DUMMY BASELINE COMPUTATION"],"metadata":{"id":"hTBEwXmJcjdS"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.dummy import DummyClassifier\n","from sklearn.metrics import f1_score, confusion_matrix\n","\n","# -----------------------------\n","# Your config (as given)\n","# -----------------------------\n","CSV_TRAIN_POOL = \"/content/drive/MyDrive/Skin_project/train_original.csv\"\n","\n","CSV_TEST_FIXED = \"/content/drive/MyDrive/Skin_project/test_original.csv\"\n","\n","FEATURE = \"moisture\"  # change as needed\n","\n","feature2imgcol = {\n","    \"moisture\": \"moisture_img\",\n","    \"oiliness\": \"oiliness_img\",\n","    \"elasticity\": \"elasticity_img\",\n","    \"texture\": \"texture_img\",\n","    \"hyperpigmentation\": \"hyperpigmentation_img\",\n","}\n","DEFAULT_LABELS = [-1, 0, 1]\n","BINARY_MAP = {\n","    \"oiliness\": { -1: 0, 0: 1, 1: 1 },  # dry vs non-dry\n","    \"moisture\": { -1: 0, 0: 1, 1: 1 },\n","}\n","FEATURE_LABELS = {\n","    \"oiliness\":  [0, 1],\n","    \"moisture\":  [0, 1],\n","    \"texture\":   [-1, 0, 1],\n","    \"elasticity\":[-1, 0, 1],\n","    \"hyperpigmentation\":[-1, 0, 1],\n","}\n","\n","# -----------------------------\n","# Helper: load labels with the same mapping you use in experiments\n","# -----------------------------\n","def load_labels(csv_path, feature):\n","    lbl_col = f\"{feature}_score\"\n","    y = (pd.read_csv(csv_path)[lbl_col]\n","         .dropna()\n","         .astype(float).round().astype(int)\n","         .values)\n","    # Apply binary mapping only when defined\n","    if feature in BINARY_MAP:\n","        y = np.array([BINARY_MAP[feature].get(int(v), v) for v in y], dtype=int)\n","    # Keep only allowed labels for this feature\n","    allowed = set(FEATURE_LABELS.get(feature, DEFAULT_LABELS))\n","    y = np.array([int(v) for v in y if int(v) in allowed], dtype=int)\n","    return y\n","\n","# -----------------------------\n","# Dummy baselines\n","# -----------------------------\n","def run_dummy_baselines(feature):\n","    y_tr = load_labels(CSV_TRAIN_POOL, feature)\n","    y_te = load_labels(CSV_TEST_FIXED, feature)\n","\n","    # Trivial features so DummyClassifier fits\n","    X_tr = np.zeros((len(y_tr), 1))\n","    X_te = np.zeros((len(y_te), 1))\n","\n","    results = {}\n","    for strat in [\"most_frequent\", \"uniform\", \"stratified\"]:\n","        d = DummyClassifier(strategy=strat, random_state=42).fit(X_tr, y_tr)\n","        yp = d.predict(X_te)\n","        macro_f1 = f1_score(y_te, yp, average=\"macro\", zero_division=0)\n","        cm = confusion_matrix(y_te, yp, labels=np.unique(y_te)).astype(float)\n","        bal_acc = (cm / cm.sum(1, keepdims=True)).diagonal().mean()\n","        results[strat] = dict(macro_f1=float(macro_f1), balanced_acc=float(bal_acc))\n","\n","    return results\n","\n","# -----------------------------\n","# Run\n","# -----------------------------\n","res = run_dummy_baselines(FEATURE)\n","for k, v in res.items():\n","    print(f\"{FEATURE:>18} | {k:13s} -> macro-F1={v['macro_f1']:.3f} | bal-acc={v['balanced_acc']:.3f}\")\n"],"metadata":{"id":"qjyCrOLsxTUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","FEATURE = \"oiliness\"\n","res = run_dummy_baselines(FEATURE)\n","for k, v in res.items():\n","    print(f\"{FEATURE:>18} | {k:13s} -> macro-F1={v['macro_f1']:.3f} | bal-acc={v['balanced_acc']:.3f}\")"],"metadata":{"id":"_3DlxaGLxYVK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FEATURE = \"elasticity\"\n","res = run_dummy_baselines(FEATURE)\n","for k, v in res.items():\n","    print(f\"{FEATURE:>18} | {k:13s} -> macro-F1={v['macro_f1']:.3f} | bal-acc={v['balanced_acc']:.3f}\")"],"metadata":{"id":"vxOVW1_SxikW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FEATURE = \"texture\"\n","res = run_dummy_baselines(FEATURE)\n","for k, v in res.items():\n","    print(f\"{FEATURE:>18} | {k:13s} -> macro-F1={v['macro_f1']:.3f} | bal-acc={v['balanced_acc']:.3f}\")"],"metadata":{"id":"bTlej6HsxmSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FEATURE = \"hyperpigmentation\"\n","res = run_dummy_baselines(FEATURE)\n","for k, v in res.items():\n","    print(f\"{FEATURE:>18} | {k:13s} -> macro-F1={v['macro_f1']:.3f} | bal-acc={v['balanced_acc']:.3f}\")"],"metadata":{"id":"7eQ5dt4KxrDu"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["5NXKrOhg9pYd","tjbe8y83IQDm","GRLLr-JELXW_"],"authorship_tag":"ABX9TyPadEhYc1Odl+YItiql8NUd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}