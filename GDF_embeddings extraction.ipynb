{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNpsF//xL/V1r+c74iHFOaO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c074a4ece6c941e58f95d011bf5f532a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3addefef5e2495e8354505a94952058","IPY_MODEL_8ef5000e49c74cab9bad849281725f18","IPY_MODEL_c4b5c177b059406bbf4242f340d76d41"],"layout":"IPY_MODEL_7ee1d83e760e4f98be423d0b9f48516e"}},"d3addefef5e2495e8354505a94952058":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a49c4cbe9fe4775b91c80fa115ef4d6","placeholder":"​","style":"IPY_MODEL_b58571d99ee14a7fb1220f57ba6087d7","value":"Fetching 7 files: 100%"}},"8ef5000e49c74cab9bad849281725f18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1126c5d78f1b47969fa69f009cf38ded","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ef05c867bb443b8897d086f5b452c6e","value":7}},"c4b5c177b059406bbf4242f340d76d41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87b2245b05074252b38e0114580103a8","placeholder":"​","style":"IPY_MODEL_153cfe507d844ea187f52f858d337cf1","value":" 7/7 [00:00&lt;00:00, 626.55it/s]"}},"7ee1d83e760e4f98be423d0b9f48516e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a49c4cbe9fe4775b91c80fa115ef4d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b58571d99ee14a7fb1220f57ba6087d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1126c5d78f1b47969fa69f009cf38ded":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ef05c867bb443b8897d086f5b452c6e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87b2245b05074252b38e0114580103a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"153cfe507d844ea187f52f858d337cf1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c4f298937ca4bf088193974fa68961f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44919c9ba76248299503f98970edfc32","IPY_MODEL_e1ee9c17be264135ba0f75a2398b2367","IPY_MODEL_2e452ee215b54d919dcbdac60127b61d"],"layout":"IPY_MODEL_f610a376a0424c6fafdc2254d418acf7"}},"44919c9ba76248299503f98970edfc32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_280aa0c4f6fc49e4b9a4655d9f185221","placeholder":"​","style":"IPY_MODEL_07e157532492410baae31391c8abe997","value":"precache: 100%"}},"e1ee9c17be264135ba0f75a2398b2367":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_21a5292fee424980beb59ca72cb5b116","max":250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90a7bee61d7d496588f690c8ab8c3c87","value":250}},"2e452ee215b54d919dcbdac60127b61d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77c3b95c186b4e88b8a18b9229da2546","placeholder":"​","style":"IPY_MODEL_f5ffef7558c04aaeab92c315ab6e8489","value":" 250/250 [32:02&lt;00:00,  7.70s/img]"}},"f610a376a0424c6fafdc2254d418acf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"280aa0c4f6fc49e4b9a4655d9f185221":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07e157532492410baae31391c8abe997":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21a5292fee424980beb59ca72cb5b116":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90a7bee61d7d496588f690c8ab8c3c87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77c3b95c186b4e88b8a18b9229da2546":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5ffef7558c04aaeab92c315ab6e8489":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgDzvcjUD_9u","executionInfo":{"status":"ok","timestamp":1757354304001,"user_tz":-60,"elapsed":868,"user":{"displayName":"Federica Dani","userId":"16887626021583890691"}},"outputId":"eee22a92-1e4b-48c0-c15e-15b1a4ce5aba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip -q install -U huggingface_hub pillow scikit-learn tqdm\n","\n","import tensorflow as tf\n","print(\"Colab TF version:\", tf.__version__)\n","\n","# (Optional) If you have a HF token, login so the hub can pull gated weights.\n","from huggingface_hub import login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T76H4tPCEOxy","executionInfo":{"status":"ok","timestamp":1757354325498,"user_tz":-60,"elapsed":18532,"user":{"displayName":"Federica Dani","userId":"16887626021583890691"}},"outputId":"22b27d89-c787-4779-b649-6f2a41fb01c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab TF version: 2.19.0\n"]}]},{"cell_type":"code","source":["# ====== Imports & basic setup =================================================\n","import os, io, time, gc, hashlib, warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","import tensorflow as tf\n","from huggingface_hub import from_pretrained_keras\n","\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold, LeaveOneGroupOut\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import (\n","    f1_score, balanced_accuracy_score, matthews_corrcoef, classification_report, confusion_matrix\n",")\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.dummy import DummyClassifier\n","from sklearn.model_selection import ParameterGrid\n","\n","# ====== Devices (Derm inference is CPU-only; head can use GPU if present) ====\n","GPUS = tf.config.list_physical_devices('GPU')\n","if GPUS:\n","    for g in GPUS:\n","        try: tf.config.experimental.set_memory_growth(g, True)\n","        except Exception: pass\n","TRAIN_DEVICE = '/GPU:0' if GPUS else '/CPU:0'\n","INFER_DEVICE = '/CPU:0'"],"metadata":{"id":"9zF_IjKUEVk9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transforming dataset from long to wide format\n","\n"],"metadata":{"id":"fzwWUqAoOhSU"}},{"cell_type":"markdown","source":["## TRAD AUGMENTATION DATASET"],"metadata":{"id":"dSLbiavwO1-y"}},{"cell_type":"code","source":["# ===================== LONG -> WIDE (robust) =====================\n","# Input (LONG):  columns ≈ patient_id, region?, feature, image_path, score\n","# Output (WIDE): columns = patient_id, region,\n","#                <feat>_img for all feats + <feat>_score for all feats\n","# Only the current feature's two columns are filled in each output row.\n","\n","import os, numpy as np, pandas as pd\n","from IPython.display import display\n","\n","# Uses your globals from the notebook:\n","# FEATURES and feature2imgcol must be defined already.\n","FEATURES_ALL = [\"moisture\",\"oiliness\",\"elasticity\",\"texture\",\"redness\",\"hyperpigmentation\"]\n","\n","# (Optional) match train_original.csv *visual* order of SCORE columns\n","SCORE_COL_ORDER = [\"moisture\",\"texture\",\"oiliness\",\"redness\",\"hyperpigmentation\",\"elasticity\"]\n","\n","def _coerce_long_schema(df):\n","    \"\"\"Rename common variants to standard long schema names.\"\"\"\n","    # normalize column names\n","    dn = {c: c.strip().lower() for c in df.columns}\n","    df = df.rename(columns=dn)\n","\n","    # synonyms for key fields\n","    rename_map = {}\n","    def first_present(*cands):\n","        for c in cands:\n","            if c in df.columns:\n","                return c\n","        return None\n","\n","    pid_col = first_present(\"patient_id\",\"patientid\",\"pid\",\"id\")\n","    feat_col = first_present(\"feature\",\"feat\")\n","    img_col  = first_present(\"image_path\",\"img_path\",\"path\",\"image\")\n","    score_col= first_present(\"score\",\"label\",\"y\",\"target\")\n","    region_col = first_present(\"region\",\"site\",\"area\")\n","\n","    missing = [n for n,(ncol) in {\"patient_id\":pid_col,\"feature\":feat_col,\"image_path\":img_col,\"score\":score_col}.items() if ncol is None]\n","    if missing:\n","        raise ValueError(f\"Input long CSV is missing required columns: {missing}\")\n","\n","    rename_map[pid_col] = \"patient_id\"\n","    rename_map[feat_col] = \"feature\"\n","    rename_map[img_col] = \"image_path\"\n","    rename_map[score_col] = \"score\"\n","    if region_col and region_col != \"region\":\n","        rename_map[region_col] = \"region\"\n","\n","    df = df.rename(columns=rename_map)\n","    if \"region\" not in df.columns:\n","        df[\"region\"] = \"\"\n","\n","    # clean values\n","    df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n","    df[\"region\"] = df[\"region\"].astype(str)\n","    df[\"feature\"] = df[\"feature\"].astype(str).str.strip().str.lower()\n","    df[\"image_path\"] = df[\"image_path\"].astype(str)\n","    # numeric score -> int, allow NaN\n","    df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\").round()\n","\n","    return df\n","\n","def long_to_wide_compatible(in_csv, out_csv, allowed_features=FEATURES_ALL, check_files=False, drop_dupes=True):\n","    \"\"\"\n","    Convert a LONG CSV to WIDE, one output row per input row (only that feature filled).\n","    - check_files=False avoids accidental row drops when Drive isn't mounted.\n","    - drop_dupes=True removes duplicate rows (same patient_id, region, feature, image_path).\n","    \"\"\"\n","    df_long = pd.read_csv(in_csv)\n","    df_long = _coerce_long_schema(df_long)\n","\n","    # keep only the features we know about\n","    df_long = df_long[df_long[\"feature\"].isin(allowed_features)].copy()\n","\n","    if drop_dupes:\n","        df_long = df_long.drop_duplicates(subset=[\"patient_id\",\"region\",\"feature\",\"image_path\"]).reset_index(drop=True)\n","\n","    # build a wide row per input\n","    rows = []\n","    for _, r in df_long.iterrows():\n","        feat = r[\"feature\"]\n","        row = {\"patient_id\": r[\"patient_id\"], \"region\": r[\"region\"]}\n","        # init all *_img and *_score columns\n","        for f in FEATURES_ALL:\n","            row[f\"{f}_img\"] = \"\"\n","            row[f\"{f}_score\"] = np.nan\n","        # fill current feature only\n","        row[f\"{feat}_img\"] = r[\"image_path\"]\n","        row[f\"{feat}_score\"] = (int(r[\"score\"]) if pd.notna(r[\"score\"]) else np.nan)\n","        rows.append(row)\n","\n","    out = pd.DataFrame(rows)\n","\n","    # exact column order (imgs then scores). Score columns in SCORE_COL_ORDER for readability\n","    img_cols   = [f\"{f}_img\" for f in FEATURES_ALL]\n","    score_cols = [f\"{f}_score\" for f in SCORE_COL_ORDER] + [f\"{f}_score\" for f in FEATURES_ALL if f not in SCORE_COL_ORDER]\n","    ordered_cols = [\"patient_id\", \"region\"] + img_cols + score_cols\n","    out = out.reindex(columns=ordered_cols)\n","\n","    # OPTIONAL: only drop rows whose SINGLE filled image path doesn't exist\n","    if check_files:\n","        def _has_valid_img(row):\n","            # find which feature is filled and check just that one\n","            for f in FEATURES_ALL:\n","                p = row[f\"{f}_img\"]\n","                if isinstance(p, str) and p:\n","                    return os.path.exists(p)\n","            return False\n","        mask = out.apply(_has_valid_img, axis=1)\n","        dropped = int((~mask).sum())\n","        out = out[mask].reset_index(drop=True)\n","        print(f\"[wide] Dropped {dropped} rows with non-existing image files (check_files=True).\")\n","\n","    out.to_csv(out_csv, index=False)\n","    print(f\"[wide] Wrote {out_csv} — rows: {len(out)}, cols: {len(out.columns)}\")\n","    return out\n","\n","def preview_wide(path, n=3):\n","    df = pd.read_csv(path)\n","    print(f\"[preview] {path} — shape={df.shape}\")\n","    display(df.head(n))\n","    # quick counts by which feature column is filled\n","    which = []\n","    for _, r in df.iterrows():\n","        fnd = None\n","        for f in FEATURES_ALL:\n","            if isinstance(r[f\"{f}_img\"], str) and r[f\"{f}_img\"]:\n","                fnd = f; break\n","        which.append(fnd)\n","    s = pd.Series(which).value_counts().sort_index()\n","    print(\"[per-feature rows]\")\n","    display(pd.DataFrame({\"rows\": s}))\n"],"metadata":{"id":"jD1x9M_wOhBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CSV_TRAD_AUG_LONG = \"/content/drive/MyDrive/Skin_project/trad_augmented_dataset.csv\"\n","CSV_TRAIN_TRAD_AUG  = \"/content/drive/MyDrive/Skin_project/trad_aug_as_wide.csv\"\n","\n","long_to_wide_compatible(CSV_TRAD_AUG_LONG,CSV_TRAIN_TRAD_AUG)\n","preview_wide(CSV_TRAIN_TRAD_AUG)"],"metadata":{"id":"IpfXtL5HOymZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DIFFUSION BASED AUGMENTATION DATASET"],"metadata":{"id":"OqDV3MjCO6t7"}},{"cell_type":"code","source":["# DIFFUSION BASED AUGM DATASET\n","\n","import os, numpy as np, pandas as pd\n","from IPython.display import display\n","\n","# Global lists (match your project)\n","FEATURES_ALL = [\"moisture\",\"oiliness\",\"elasticity\",\"texture\",\"redness\",\"hyperpigmentation\"]\n","SCORE_COL_ORDER = [\"moisture\",\"texture\",\"oiliness\",\"redness\",\"hyperpigmentation\",\"elasticity\"]\n","\n","# --- helpers ---\n","\n","def _coerce_long_schema(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Normalize a 'long' CSV to canonical column names & types.\"\"\"\n","    # 1) normalize headers\n","    df = df.rename(columns={c: c.strip().lower() for c in df.columns})\n","\n","    def first_present(*cands):\n","        for c in cands:\n","            if c in df.columns: return c\n","        return None\n","\n","    pid_col   = first_present(\"patient_id\",\"patientid\",\"pid\",\"id\")\n","    feat_col  = first_present(\"feature\",\"feat\")\n","    path_col  = first_present(\"image_path\",\"img_path\",\"path\",\"image\")\n","    score_col = first_present(\"score\",\"label\",\"y\",\"target\")\n","    region_col= first_present(\"region\",\"site\",\"area\")\n","    source_col= first_present(\"source\",\"origin\",\"dataset\",\"provider\")\n","\n","    missing = [n for n, col in {\n","        \"patient_id\": pid_col, \"feature\": feat_col,\n","        \"image_path\": path_col, \"score\": score_col\n","    }.items() if col is None]\n","    if missing:\n","        raise ValueError(f\"Input long CSV is missing required columns: {missing}\")\n","\n","    rename_map = {\n","        pid_col: \"patient_id\",\n","        feat_col: \"feature\",\n","        path_col: \"image_path\",\n","        score_col: \"score\",\n","    }\n","    if region_col and region_col != \"region\": rename_map[region_col] = \"region\"\n","    if source_col and source_col != \"source\": rename_map[source_col] = \"source\"\n","\n","    df = df.rename(columns=rename_map)\n","\n","    # 2) clean values\n","    if \"region\" not in df.columns:\n","        df[\"region\"] = \"\"\n","    if \"source\" not in df.columns:\n","        df[\"source\"] = \"\"\n","\n","    df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n","    df[\"region\"]     = df[\"region\"].astype(str)\n","    df[\"feature\"]    = df[\"feature\"].astype(str).str.strip().str.lower()\n","    df[\"image_path\"] = df[\"image_path\"].astype(str)\n","    df[\"source\"]     = df[\"source\"].astype(str)\n","\n","    # robust score → int {-1,0,1}\n","    _label_map = {\"low\": -1, \"avg\": 0, \"average\": 0, \"med\": 0, \"medium\": 0, \"high\": 1}\n","    def _to_int(v):\n","        try: return int(round(float(v)))\n","        except: return _label_map.get(str(v).strip().lower(), 0)\n","    df[\"score\"] = df[\"score\"].apply(_to_int)\n","\n","    return df\n","\n","\n","def long_to_wide_compatible(\n","    in_csv: str,\n","    out_csv: str,\n","    allowed_features=FEATURES_ALL,\n","    check_files: bool=False,\n","    drop_dupes: bool=True,\n","    keep_source: bool=True,\n","    source_col_name: str=\"source\",\n","):\n","    \"\"\"\n","    Convert a LONG CSV (one row per (patient, region, feature, path, score[, source]))\n","    into a WIDE CSV where each row has columns:\n","      patient_id, region, [<feat>_img]*6, [<feat>_score]*6 [, source]\n","    Only the current feature's *_img and *_score are filled per row.\n","    \"\"\"\n","    df_long = pd.read_csv(in_csv)\n","    df_long = _coerce_long_schema(df_long)\n","\n","    # restrict to known features\n","    df_long = df_long[df_long[\"feature\"].isin(allowed_features)].copy()\n","\n","    if drop_dupes:\n","        df_long = df_long.drop_duplicates(\n","            subset=[\"patient_id\",\"region\",\"feature\",\"image_path\"]\n","        ).reset_index(drop=True)\n","\n","    rows = []\n","    has_source = keep_source and (source_col_name in df_long.columns)\n","    for _, r in df_long.iterrows():\n","        feat = r[\"feature\"]\n","        row = {\"patient_id\": r[\"patient_id\"], \"region\": r[\"region\"]}\n","        # init all *_img and *_score\n","        for f in FEATURES_ALL:\n","            row[f\"{f}_img\"] = \"\"\n","            row[f\"{f}_score\"] = np.nan\n","        # fill only this feature\n","        row[f\"{feat}_img\"]   = r[\"image_path\"]\n","        row[f\"{feat}_score\"] = int(r[\"score\"])\n","        if has_source:\n","            row[source_col_name] = r[source_col_name]\n","        rows.append(row)\n","\n","    out = pd.DataFrame(rows)\n","\n","    # Column order\n","    img_cols   = [f\"{f}_img\" for f in FEATURES_ALL]\n","    score_cols = [f\"{f}_score\" for f in SCORE_COL_ORDER] + \\\n","                 [f\"{f}_score\" for f in FEATURES_ALL if f not in SCORE_COL_ORDER]\n","    ordered = [\"patient_id\",\"region\"] + img_cols + score_cols\n","    if has_source: ordered.append(source_col_name)\n","    out = out.reindex(columns=ordered)\n","\n","    if check_files:\n","        def _has_valid_img(row):\n","            for f in FEATURES_ALL:\n","                p = row[f\"{f}_img\"]\n","                if isinstance(p, str) and p:\n","                    return os.path.exists(p)\n","            return False\n","        mask = out.apply(_has_valid_img, axis=1)\n","        dropped = int((~mask).sum())\n","        out = out[mask].reset_index(drop=True)\n","        print(f\"[wide] Dropped {dropped} rows with non-existing image files.\")\n","\n","    out.to_csv(out_csv, index=False)\n","    print(f\"[wide] Wrote {out_csv} — rows={len(out)}, cols={len(out.columns)}\")\n","    return out\n","\n","\n","def preview_wide(path, n=5):\n","    df = pd.read_csv(path)\n","    print(f\"[preview] {path} — shape={df.shape}\")\n","    display(df.head(n))\n","    # per-feature row counts (which feature is filled)\n","    which = []\n","    for _, r in df.iterrows():\n","        fnd = None\n","        for f in FEATURES_ALL:\n","            if isinstance(r.get(f\"{f}_img\",\"\"), str) and r[f\"{f}_img\"]:\n","                fnd = f; break\n","        which.append(fnd)\n","    s = pd.Series(which, dtype=\"object\").value_counts().sort_index()\n","    print(\"[per-feature rows]\"); display(pd.DataFrame({\"rows\": s}))\n"],"metadata":{"id":"hQnS0tapPC7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CSV_SYN_LONG = \"/content/drive/MyDrive/Skin_project/dataset_generated_only.csv\"\n","CSV_SYN_WIDE = \"/content/drive/MyDrive/Skin_project/synthetic_as_wide.csv\"\n","\n","_ = long_to_wide_compatible(\n","        CSV_SYN_LONG, CSV_SYN_WIDE,\n","        check_files=True,     # set False if Drive isn't mounted yet\n","        keep_source=True      # keeps 'source' column (\"synthetic\")\n","    )\n","preview_wide(CSV_SYN_WIDE)\n"],"metadata":{"id":"QJA-Otc_PDie"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GDF Embeddigns Extraction code"],"metadata":{"id":"_ooYULQ2PHQx"}},{"cell_type":"code","source":["# ====== Paths / config ========================================================\n","CSV_TRAIN_POOL   = \"/content/drive/MyDrive/Skin_project/train_original.csv\"\n","#CSV_TRAIN_TRAD   = \"/content/drive/MyDrive/Skin_project/trad_aug_as_wide.csv\"\n","CSV_SYN_WIDE     = \"/content/drive/MyDrive/Skin_project/synthetic_as_wide.csv\"\n","CSV_TEST_FIXED   = \"/content/drive/MyDrive/Skin_project/test_original.csv\"\n","\n","# Keep redness out for now\n","FEATURES = [\"texture\",\"hyperpigmentation\",\"oiliness\",\"moisture\",\"elasticity\"]\n","\n","feature2imgcol = {\n","    \"moisture\": \"moisture_img\",\n","    \"oiliness\": \"oiliness_img\",\n","    \"elasticity\": \"elasticity_img\",\n","    \"texture\": \"texture_img\",\n","    \"redness\": \"redness_img\",\n","    \"hyperpigmentation\": \"hyperpigmentation_img\",\n","}\n","\n","# Base labels\n","DEFAULT_LABELS = [-1, 0, 1]\n","\n","# Binary remaps for the two very-imbalanced tasks\n","BINARY_MAP = {\n","    \"oiliness\": { -1: 0, 0: 1, 1: 1 },   # dry vs non-dry\n","    \"moisture\": { -1: 0, 0: 1, 1: 1 },   # low vs non-low\n","}\n","FEATURE_LABELS = {\n","    \"oiliness\":  [0, 1],\n","    \"moisture\":  [0, 1],\n","    # others keep DEFAULT_LABELS\n","}\n","\n","VAL_FRAC_WITHIN_TRAIN = 0.125\n","RANDOM_SEED = 42\n","\n","# Embeddings cache\n","CACHE_DIR = \"/content/drive/MyDrive/Skin_project/derm_emb_cache\"\n","os.makedirs(CACHE_DIR, exist_ok=True)\n","\n","BATCH_SIZE_INFER = 128\n","NUM_WORKERS      = min(8, os.cpu_count() or 4)"],"metadata":{"id":"NiuFpGr2EWoq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ====== Derm foundation model (inference signature only) =====================\n","derm_model = from_pretrained_keras(\"google/derm-foundation\")\n","derm_infer = derm_model.signatures[\"serving_default\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["c074a4ece6c941e58f95d011bf5f532a","d3addefef5e2495e8354505a94952058","8ef5000e49c74cab9bad849281725f18","c4b5c177b059406bbf4242f340d76d41","7ee1d83e760e4f98be423d0b9f48516e","5a49c4cbe9fe4775b91c80fa115ef4d6","b58571d99ee14a7fb1220f57ba6087d7","1126c5d78f1b47969fa69f009cf38ded","2ef05c867bb443b8897d086f5b452c6e","87b2245b05074252b38e0114580103a8","153cfe507d844ea187f52f858d337cf1"]},"id":"e45kMo9-EdGM","executionInfo":{"status":"ok","timestamp":1757354363492,"user_tz":-60,"elapsed":15112,"user":{"displayName":"Federica Dani","userId":"16887626021583890691"}},"outputId":"f29dadb8-81b5-43a3-c4c3-6426dcaf7d33"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c074a4ece6c941e58f95d011bf5f532a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"]}]},{"cell_type":"code","source":["\n","# Tune these if needed (smaller = less RAM)\n","ENCODE_WORKERS = 1            # threads that read & PNG-encode\n","ENCODE_CHUNK   = 64         # how many images to encode before inferring\n","STREAM_BATCH   = 32        # inference batch size for Derm (CPU)"],"metadata":{"id":"gak9r9wMZKml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _chunked(it, n):\n","    for i in range(0, len(it), n):\n","        yield it[i:i+n]\n"],"metadata":{"id":"rD7GX98zeEg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ====== Small progress helper =================================================\n","def make_phase_bar(desc, phases, position=1, leave=True):\n","    bar = tqdm(total=len(phases), desc=desc, position=position, leave=leave)\n","    idx = {\"i\": 0, \"phases\": phases}\n","    def tick(label=None):\n","        if label is None and idx[\"i\"] < len(idx[\"phases\"]):\n","            label = idx[\"phases\"][idx[\"i\"]]\n","        if label is not None: bar.set_postfix_str(str(label))\n","        bar.update(1); idx[\"i\"] += 1\n","    def close(): bar.close()\n","    return bar, tick, close\n","\n","# ====== Embedding I/O helpers =================================================\n","def _png_bytes_from_pil(pil_img: Image.Image) -> bytes:\n","    buf = io.BytesIO()\n","    pil_img.convert(\"RGB\").save(buf, format=\"PNG\")\n","    return buf.getvalue()\n","\n","def _example_from_png_bytes(image_bytes: bytes) -> bytes:\n","    ex = tf.train.Example(features=tf.train.Features(\n","        feature={'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))}\n","    ))\n","    return ex.SerializeToString()\n","\n","def _sha1(s: str) -> str:\n","    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n","\n","def _emb_cache_path(img_path: str) -> str:\n","    return os.path.join(CACHE_DIR, _sha1(img_path) + \".npy\")\n","\n","def _load_embed_if_cached(img_path: str):\n","    fp = _emb_cache_path(img_path)\n","    if os.path.exists(fp):\n","        try:\n","            arr = np.load(fp, allow_pickle=False)\n","            if arr.ndim == 1 and arr.shape[0] == 6144 and arr.dtype == np.float32:\n","                return arr\n","        except Exception:\n","            pass\n","    return None\n","\n","def _save_embed_to_cache(img_path: str, emb: np.ndarray):\n","    fp = _emb_cache_path(img_path)\n","    try: np.save(fp, emb.astype(np.float32), allow_pickle=False)\n","    except Exception: pass\n","\n","def _encode_one_path(img_path: str):\n","    try:\n","        with Image.open(img_path) as im:\n","            b = _png_bytes_from_pil(im)\n","        return img_path, _example_from_png_bytes(b)\n","    except Exception:\n","        return img_path, None\n","\n","def build_embeddings_cache(paths, desc=\"build cache\", position=2, allow_infer=True,\n","                           encode_workers=ENCODE_WORKERS, encode_chunk=ENCODE_CHUNK,\n","                           stream_batch=STREAM_BATCH, return_map=True):\n","    \"\"\"\n","    Streamed, RAM-safe cache builder.\n","    If return_map=False, embeddings are NOT accumulated in RAM (best for pre-cache).\n","    \"\"\"\n","    # Only unique, existing paths\n","    paths = [p for p in paths if isinstance(p, str) and os.path.exists(p)]\n","    uniq  = sorted(set(paths))\n","\n","    cache_map, missing = ({}, [])\n","    for p in uniq:\n","        emb = _load_embed_if_cached(p)\n","        if emb is not None:\n","            if return_map:\n","                cache_map[p] = emb\n","        else:\n","            missing.append(p)\n","\n","    if not missing or not allow_infer:\n","        if missing and not allow_infer:\n","            print(f\"⚠️ {len(missing)} missing embeddings; not inferring (allow_infer=False).\")\n","        return cache_map\n","\n","    # ---- encode + infer in bounded chunks ----\n","    pbar = tqdm(total=len(missing), desc=f\"{desc}\", unit=\"img\", position=position)\n","    for chunk_paths in _chunked(missing, encode_chunk):\n","        # Encode this chunk (bounded)\n","        encoded = []\n","        with ThreadPoolExecutor(max_workers=encode_workers) as ex:\n","            futs = {ex.submit(_encode_one_path, p): p for p in chunk_paths}\n","            for fut in as_completed(futs):\n","                p, ex_bytes = fut.result()\n","                if ex_bytes is not None:\n","                    encoded.append((p, ex_bytes))\n","\n","        # Infer this chunk in small batches and immediately write to disk\n","        for i in range(0, len(encoded), stream_batch):\n","            batch = encoded[i:i+stream_batch]\n","            if not batch:\n","                continue\n","            batch_paths = [p for p, _ in batch]\n","            batch_exs   = [exb for _, exb in batch]\n","            with tf.device(INFER_DEVICE):\n","                outs = derm_infer(inputs=tf.constant(batch_exs))\n","                embs = outs[\"embedding\"].numpy().astype(np.float32)\n","\n","            for p, e in zip(batch_paths, embs):\n","                # save to disk first\n","                _save_embed_to_cache(p, e)\n","                # keep in RAM only if requested\n","                if return_map:\n","                    cache_map[p] = e\n","\n","            # aggressively free batch buffers\n","            del batch, batch_paths, batch_exs, embs\n","            gc.collect()\n","\n","        # free the encoded chunk\n","        del encoded\n","        gc.collect()\n","\n","        pbar.update(len(chunk_paths))\n","    pbar.close()\n","    return cache_map\n","\n","\n","\n","def clear_embedding_cache():\n","    if not os.path.isdir(CACHE_DIR):\n","        print(\"No cache dir to clear.\"); return\n","    n = 0\n","    for fn in os.listdir(CACHE_DIR):\n","        if fn.endswith(\".npy\"):\n","            try:\n","                os.remove(os.path.join(CACHE_DIR, fn)); n += 1\n","            except Exception: pass\n","    print(f\"Cleared {n} cached files from {CACHE_DIR}.\")\n","\n","# ====== Data loading (handles binary remaps) =================================\n","def load_feature_df_one(csv_path, feature):\n","    df_all  = pd.read_csv(csv_path)\n","    img_col = feature2imgcol[feature]\n","    lbl_col = f\"{feature}_score\"\n","    cols    = [\"patient_id\", img_col, lbl_col] + ([\"region\"] if \"region\" in df_all.columns else [])\n","    df      = df_all[cols].copy()\n","    df      = df.rename(columns={img_col: \"image_path\", lbl_col: \"label\"})\n","\n","    # Keep rows that actually have an image on disk\n","    df = df[df[\"image_path\"].apply(lambda p: isinstance(p, str) and os.path.exists(p))]\n","    df = df.dropna(subset=[\"label\"])\n","    df[\"label\"] = df[\"label\"].astype(float).round().astype(int)\n","\n","    # Optional binary remap (oiliness/moisture)\n","    if feature in BINARY_MAP:\n","        df[\"label\"] = df[\"label\"].map(BINARY_MAP[feature])\n","        df = df.dropna(subset=[\"label\"]).astype({\"label\": int})\n","\n","    allowed = FEATURE_LABELS.get(feature, DEFAULT_LABELS)\n","    df = df[df[\"label\"].isin(allowed)].reset_index(drop=True)\n","    return df\n","\n","def load_feature_df_multi(csv_paths, feature):\n","    frames = [load_feature_df_one(p, feature) for p in csv_paths]\n","    df = pd.concat(frames, ignore_index=True)\n","    df = df.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n","    return df\n","\n","def df_to_Xy(df, emb_map):\n","    keep = [i for i, p in enumerate(df.image_path.values) if p in emb_map]\n","    if not keep:\n","        return np.zeros((0, 6144), np.float32), np.zeros((0,), np.int64), df.iloc[[]]\n","    df2 = df.iloc[keep].reset_index(drop=True)\n","    X   = np.stack([emb_map[p] for p in df2.image_path.values], axis=0)\n","    y   = df2.label.values.astype(np.int64)\n","    return X, y, df2\n","\n","def remove_patient_overlap(df_trainpool, df_test):\n","    overlap = set(df_trainpool.patient_id.unique()) & set(df_test.patient_id.unique())\n","    if overlap:\n","        print(f\"⚠️ Removing {len(overlap)} overlapping patient_id(s) to prevent leakage.\")\n","        df_trainpool = df_trainpool[~df_trainpool.patient_id.isin(overlap)].reset_index(drop=True)\n","    return df_trainpool\n","\n","def make_val_from_trainpool(df_trainpool, val_frac_within_train=VAL_FRAC_WITHIN_TRAIN, seed=RANDOM_SEED):\n","    groups = df_trainpool[\"patient_id\"].values\n","    gss    = GroupShuffleSplit(n_splits=1, test_size=val_frac_within_train, random_state=seed)\n","    tr_idx, va_idx = next(gss.split(df_trainpool, groups=groups))\n","    return df_trainpool.iloc[tr_idx].reset_index(drop=True), df_trainpool.iloc[va_idx].reset_index(drop=True)\n","\n","# ====== Precache convenience ==================================================\n","#def precache_from_csvs(features, train_csvs, include_test=True, clear_cache=False):\n","    #if clear_cache: clear_embedding_cache()\n","    #paths = []\n","    #for feat in features:\n","        #for csvp in train_csvs:\n","           # df_trp = load_feature_df_one(csvp, feat); paths.extend(df_trp.image_path.values)\n","        #if include_test:\n","            #df_te = load_feature_df_one(CSV_TEST_FIXED, feat); paths.extend(df_te.image_path.values)\n","    #uniq = sorted({p for p in paths if isinstance(p, str) and os.path.exists(p)})\n","    #t0 = time.time()\n","    #cached_before = sum(os.path.exists(_emb_cache_path(p)) for p in uniq)\n","    #build_embeddings_cache(uniq, desc=\"precache\", allow_infer=True)\n","    #cached_after  = sum(os.path.exists(_emb_cache_path(p)) for p in uniq)\n","    #print(f\"Pre-cache: {len(uniq)} unique images | cached {cached_after} (new {cached_after-cached_before}) in {time.time()-t0:.1f}s.\")\n","\n","def precache_from_csvs(features, train_csvs, include_test=True, clear_cache=False):\n","    if clear_cache: clear_embedding_cache()\n","    paths = []\n","    for feat in features:\n","        for csvp in train_csvs:\n","            df_trp = load_feature_df_one(csvp, feat); paths.extend(df_trp.image_path.values)\n","        if include_test:\n","            df_te = load_feature_df_one(CSV_TEST_FIXED, feat); paths.extend(df_te.image_path.values)\n","\n","    uniq = sorted({p for p in paths if isinstance(p, str) and os.path.exists(p)})\n","\n","    t0 = time.time()\n","    cached_before = sum(os.path.exists(_emb_cache_path(p)) for p in uniq)\n","\n","    # STREAMED + DO NOT ACCUMULATE IN RAM\n","    build_embeddings_cache(\n","        uniq, desc=\"precache\", allow_infer=True,\n","        encode_workers=ENCODE_WORKERS, encode_chunk=ENCODE_CHUNK,\n","        stream_batch=STREAM_BATCH, return_map=False\n","    )\n","\n","    cached_after  = sum(os.path.exists(_emb_cache_path(p)) for p in uniq)\n","    print(f\"Pre-cache: {len(uniq)} unique images | cached {cached_after} (new {cached_after-cached_before}) in {time.time()-t0:.1f}s.\")\n"],"metadata":{"id":"AvGErO83Efot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9PXyX6BpeQcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1) Choosing which CSVs to use in the training pool\n","\n","\n","#TRAIN_CSVS = [CSV_TRAIN_POOL, CSV_TRAIN_TRAD]  # real + trad aug\n","TRAIN_CSVS = [CSV_TRAIN_POOL, CSV_SYN_WIDE]    # real + diffusion"],"metadata":{"id":"NDANOxxqM1Na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CACHE_DIR = \"/content/drive/MyDrive/Skin_project/derm_emb_cache\"\n","os.makedirs(CACHE_DIR, exist_ok=True)"],"metadata":{"id":"tjS_d7sMPyDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2) warm the cache once\n","precache_from_csvs(FEATURES, TRAIN_CSVS, include_test=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["5c4f298937ca4bf088193974fa68961f","44919c9ba76248299503f98970edfc32","e1ee9c17be264135ba0f75a2398b2367","2e452ee215b54d919dcbdac60127b61d","f610a376a0424c6fafdc2254d418acf7","280aa0c4f6fc49e4b9a4655d9f185221","07e157532492410baae31391c8abe997","21a5292fee424980beb59ca72cb5b116","90a7bee61d7d496588f690c8ab8c3c87","77c3b95c186b4e88b8a18b9229da2546","f5ffef7558c04aaeab92c315ab6e8489"]},"id":"jhMKX7qrM145","executionInfo":{"status":"ok","timestamp":1757356306224,"user_tz":-60,"elapsed":1924710,"user":{"displayName":"Federica Dani","userId":"16887626021583890691"}},"outputId":"8339121a-c83e-4714-f987-1b13d686cb33"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["precache:   0%|          | 0/250 [00:00<?, ?img/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c4f298937ca4bf088193974fa68961f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Pre-cache: 635 unique images | cached 635 (new 250) in 1924.2s.\n"]}]}]}