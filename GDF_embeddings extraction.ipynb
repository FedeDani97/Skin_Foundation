{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDxeet0HsaNtFGcD83+br0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# GOOGLE DERM FOUNDATION MODEL - EMBEDDING EXTRACTION"],"metadata":{"id":"S1J48ZHAEwcB"}},{"cell_type":"markdown","source":["Implementing the GDF model to run separately because it runs on CPU"],"metadata":{"id":"FlcrgyLPE63Q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgDzvcjUD_9u"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip -q install -U huggingface_hub pillow scikit-learn tqdm\n","\n","import tensorflow as tf\n","print(\"Colab TF version:\", tf.__version__)\n","\n","from huggingface_hub import login"],"metadata":{"id":"T76H4tPCEOxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ====== Imports & basic setup =================================================\n","import os, io, time, gc, hashlib, warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","import tensorflow as tf\n","from huggingface_hub import from_pretrained_keras\n","\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold, LeaveOneGroupOut\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import (\n","    f1_score, balanced_accuracy_score, matthews_corrcoef, classification_report, confusion_matrix\n",")\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.dummy import DummyClassifier\n","from sklearn.model_selection import ParameterGrid\n","\n","# ====== Devices (Derm inference is CPU-only; head can use GPU if present) ====\n","GPUS = tf.config.list_physical_devices('GPU')\n","if GPUS:\n","    for g in GPUS:\n","        try: tf.config.experimental.set_memory_growth(g, True)\n","        except Exception: pass\n","TRAIN_DEVICE = '/GPU:0' if GPUS else '/CPU:0'\n","INFER_DEVICE = '/CPU:0'"],"metadata":{"id":"9zF_IjKUEVk9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transforming dataset from long to wide format\n","\n"],"metadata":{"id":"fzwWUqAoOhSU"}},{"cell_type":"markdown","source":["## TRAD AUGMENTATION DATASET"],"metadata":{"id":"dSLbiavwO1-y"}},{"cell_type":"code","source":["# ===================== LONG -> WIDE (robust) =====================\n","# Input (LONG):  columns ≈ patient_id, region?, feature, image_path, score\n","# Output (WIDE): columns = patient_id, region,\n","#                <feat>_img for all feats + <feat>_score for all feats\n","# Only the current feature's two columns are filled in each output row.\n","\n","import os, numpy as np, pandas as pd\n","from IPython.display import display\n","\n","# Uses your globals from the notebook:\n","# FEATURES and feature2imgcol must be defined already.\n","FEATURES_ALL = [\"moisture\",\"oiliness\",\"elasticity\",\"texture\",\"redness\",\"hyperpigmentation\"]\n","\n","# (Optional) match train_original.csv *visual* order of SCORE columns\n","SCORE_COL_ORDER = [\"moisture\",\"texture\",\"oiliness\",\"redness\",\"hyperpigmentation\",\"elasticity\"]\n","\n","def _coerce_long_schema(df):\n","    \"\"\"Rename common variants to standard long schema names.\"\"\"\n","    # normalize column names\n","    dn = {c: c.strip().lower() for c in df.columns}\n","    df = df.rename(columns=dn)\n","\n","    # synonyms for key fields\n","    rename_map = {}\n","    def first_present(*cands):\n","        for c in cands:\n","            if c in df.columns:\n","                return c\n","        return None\n","\n","    pid_col = first_present(\"patient_id\",\"patientid\",\"pid\",\"id\")\n","    feat_col = first_present(\"feature\",\"feat\")\n","    img_col  = first_present(\"image_path\",\"img_path\",\"path\",\"image\")\n","    score_col= first_present(\"score\",\"label\",\"y\",\"target\")\n","    region_col = first_present(\"region\",\"site\",\"area\")\n","\n","    missing = [n for n,(ncol) in {\"patient_id\":pid_col,\"feature\":feat_col,\"image_path\":img_col,\"score\":score_col}.items() if ncol is None]\n","    if missing:\n","        raise ValueError(f\"Input long CSV is missing required columns: {missing}\")\n","\n","    rename_map[pid_col] = \"patient_id\"\n","    rename_map[feat_col] = \"feature\"\n","    rename_map[img_col] = \"image_path\"\n","    rename_map[score_col] = \"score\"\n","    if region_col and region_col != \"region\":\n","        rename_map[region_col] = \"region\"\n","\n","    df = df.rename(columns=rename_map)\n","    if \"region\" not in df.columns:\n","        df[\"region\"] = \"\"\n","\n","    # clean values\n","    df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n","    df[\"region\"] = df[\"region\"].astype(str)\n","    df[\"feature\"] = df[\"feature\"].astype(str).str.strip().str.lower()\n","    df[\"image_path\"] = df[\"image_path\"].astype(str)\n","    # numeric score -> int, allow NaN\n","    df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\").round()\n","\n","    return df\n","\n","def long_to_wide_compatible(in_csv, out_csv, allowed_features=FEATURES_ALL, check_files=False, drop_dupes=True):\n","    \"\"\"\n","    Convert a LONG CSV to WIDE, one output row per input row (only that feature filled).\n","    - check_files=False avoids accidental row drops when Drive isn't mounted.\n","    - drop_dupes=True removes duplicate rows (same patient_id, region, feature, image_path).\n","    \"\"\"\n","    df_long = pd.read_csv(in_csv)\n","    df_long = _coerce_long_schema(df_long)\n","\n","    # keep only the features we know about\n","    df_long = df_long[df_long[\"feature\"].isin(allowed_features)].copy()\n","\n","    if drop_dupes:\n","        df_long = df_long.drop_duplicates(subset=[\"patient_id\",\"region\",\"feature\",\"image_path\"]).reset_index(drop=True)\n","\n","    # build a wide row per input\n","    rows = []\n","    for _, r in df_long.iterrows():\n","        feat = r[\"feature\"]\n","        row = {\"patient_id\": r[\"patient_id\"], \"region\": r[\"region\"]}\n","        # init all *_img and *_score columns\n","        for f in FEATURES_ALL:\n","            row[f\"{f}_img\"] = \"\"\n","            row[f\"{f}_score\"] = np.nan\n","        # fill current feature only\n","        row[f\"{feat}_img\"] = r[\"image_path\"]\n","        row[f\"{feat}_score\"] = (int(r[\"score\"]) if pd.notna(r[\"score\"]) else np.nan)\n","        rows.append(row)\n","\n","    out = pd.DataFrame(rows)\n","\n","    # exact column order (imgs then scores). Score columns in SCORE_COL_ORDER for readability\n","    img_cols   = [f\"{f}_img\" for f in FEATURES_ALL]\n","    score_cols = [f\"{f}_score\" for f in SCORE_COL_ORDER] + [f\"{f}_score\" for f in FEATURES_ALL if f not in SCORE_COL_ORDER]\n","    ordered_cols = [\"patient_id\", \"region\"] + img_cols + score_cols\n","    out = out.reindex(columns=ordered_cols)\n","\n","    # OPTIONAL: only drop rows whose SINGLE filled image path doesn't exist\n","    if check_files:\n","        def _has_valid_img(row):\n","            # find which feature is filled and check just that one\n","            for f in FEATURES_ALL:\n","                p = row[f\"{f}_img\"]\n","                if isinstance(p, str) and p:\n","                    return os.path.exists(p)\n","            return False\n","        mask = out.apply(_has_valid_img, axis=1)\n","        dropped = int((~mask).sum())\n","        out = out[mask].reset_index(drop=True)\n","        print(f\"[wide] Dropped {dropped} rows with non-existing image files (check_files=True).\")\n","\n","    out.to_csv(out_csv, index=False)\n","    print(f\"[wide] Wrote {out_csv} — rows: {len(out)}, cols: {len(out.columns)}\")\n","    return out\n","\n","def preview_wide(path, n=3):\n","    df = pd.read_csv(path)\n","    print(f\"[preview] {path} — shape={df.shape}\")\n","    display(df.head(n))\n","    # quick counts by which feature column is filled\n","    which = []\n","    for _, r in df.iterrows():\n","        fnd = None\n","        for f in FEATURES_ALL:\n","            if isinstance(r[f\"{f}_img\"], str) and r[f\"{f}_img\"]:\n","                fnd = f; break\n","        which.append(fnd)\n","    s = pd.Series(which).value_counts().sort_index()\n","    print(\"[per-feature rows]\")\n","    display(pd.DataFrame({\"rows\": s}))\n"],"metadata":{"id":"jD1x9M_wOhBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CSV_TRAD_AUG_LONG = \"/content/drive/MyDrive/Skin_project/trad_augmented_dataset.csv\"\n","CSV_TRAIN_TRAD_AUG  = \"/content/drive/MyDrive/Skin_project/trad_aug_as_wide.csv\"\n","\n","long_to_wide_compatible(CSV_TRAD_AUG_LONG,CSV_TRAIN_TRAD_AUG)\n","preview_wide(CSV_TRAIN_TRAD_AUG)"],"metadata":{"id":"IpfXtL5HOymZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DIFFUSION BASED AUGMENTATION DATASET"],"metadata":{"id":"OqDV3MjCO6t7"}},{"cell_type":"code","source":["# DIFFUSION BASED AUGM DATASET\n","\n","import os, numpy as np, pandas as pd\n","from IPython.display import display\n","\n","# Global lists (match your project)\n","FEATURES_ALL = [\"moisture\",\"oiliness\",\"elasticity\",\"texture\",\"redness\",\"hyperpigmentation\"]\n","SCORE_COL_ORDER = [\"moisture\",\"texture\",\"oiliness\",\"redness\",\"hyperpigmentation\",\"elasticity\"]\n","\n","# --- helpers ---\n","\n","def _coerce_long_schema(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Normalize a 'long' CSV to canonical column names & types.\"\"\"\n","    # 1) normalize headers\n","    df = df.rename(columns={c: c.strip().lower() for c in df.columns})\n","\n","    def first_present(*cands):\n","        for c in cands:\n","            if c in df.columns: return c\n","        return None\n","\n","    pid_col   = first_present(\"patient_id\",\"patientid\",\"pid\",\"id\")\n","    feat_col  = first_present(\"feature\",\"feat\")\n","    path_col  = first_present(\"image_path\",\"img_path\",\"path\",\"image\")\n","    score_col = first_present(\"score\",\"label\",\"y\",\"target\")\n","    region_col= first_present(\"region\",\"site\",\"area\")\n","    source_col= first_present(\"source\",\"origin\",\"dataset\",\"provider\")\n","\n","    missing = [n for n, col in {\n","        \"patient_id\": pid_col, \"feature\": feat_col,\n","        \"image_path\": path_col, \"score\": score_col\n","    }.items() if col is None]\n","    if missing:\n","        raise ValueError(f\"Input long CSV is missing required columns: {missing}\")\n","\n","    rename_map = {\n","        pid_col: \"patient_id\",\n","        feat_col: \"feature\",\n","        path_col: \"image_path\",\n","        score_col: \"score\",\n","    }\n","    if region_col and region_col != \"region\": rename_map[region_col] = \"region\"\n","    if source_col and source_col != \"source\": rename_map[source_col] = \"source\"\n","\n","    df = df.rename(columns=rename_map)\n","\n","    # 2) clean values\n","    if \"region\" not in df.columns:\n","        df[\"region\"] = \"\"\n","    if \"source\" not in df.columns:\n","        df[\"source\"] = \"\"\n","\n","    df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n","    df[\"region\"]     = df[\"region\"].astype(str)\n","    df[\"feature\"]    = df[\"feature\"].astype(str).str.strip().str.lower()\n","    df[\"image_path\"] = df[\"image_path\"].astype(str)\n","    df[\"source\"]     = df[\"source\"].astype(str)\n","\n","    # robust score → int {-1,0,1}\n","    _label_map = {\"low\": -1, \"avg\": 0, \"average\": 0, \"med\": 0, \"medium\": 0, \"high\": 1}\n","    def _to_int(v):\n","        try: return int(round(float(v)))\n","        except: return _label_map.get(str(v).strip().lower(), 0)\n","    df[\"score\"] = df[\"score\"].apply(_to_int)\n","\n","    return df\n","\n","\n","def long_to_wide_compatible(\n","    in_csv: str,\n","    out_csv: str,\n","    allowed_features=FEATURES_ALL,\n","    check_files: bool=False,\n","    drop_dupes: bool=True,\n","    keep_source: bool=True,\n","    source_col_name: str=\"source\",\n","):\n","    \"\"\"\n","    Convert a LONG CSV (one row per (patient, region, feature, path, score[, source]))\n","    into a WIDE CSV where each row has columns:\n","      patient_id, region, [<feat>_img]*6, [<feat>_score]*6 [, source]\n","    Only the current feature's *_img and *_score are filled per row.\n","    \"\"\"\n","    df_long = pd.read_csv(in_csv)\n","    df_long = _coerce_long_schema(df_long)\n","\n","    # restrict to known features\n","    df_long = df_long[df_long[\"feature\"].isin(allowed_features)].copy()\n","\n","    if drop_dupes:\n","        df_long = df_long.drop_duplicates(\n","            subset=[\"patient_id\",\"region\",\"feature\",\"image_path\"]\n","        ).reset_index(drop=True)\n","\n","    rows = []\n","    has_source = keep_source and (source_col_name in df_long.columns)\n","    for _, r in df_long.iterrows():\n","        feat = r[\"feature\"]\n","        row = {\"patient_id\": r[\"patient_id\"], \"region\": r[\"region\"]}\n","        # init all *_img and *_score\n","        for f in FEATURES_ALL:\n","            row[f\"{f}_img\"] = \"\"\n","            row[f\"{f}_score\"] = np.nan\n","        # fill only this feature\n","        row[f\"{feat}_img\"]   = r[\"image_path\"]\n","        row[f\"{feat}_score\"] = int(r[\"score\"])\n","        if has_source:\n","            row[source_col_name] = r[source_col_name]\n","        rows.append(row)\n","\n","    out = pd.DataFrame(rows)\n","\n","    # Column order\n","    img_cols   = [f\"{f}_img\" for f in FEATURES_ALL]\n","    score_cols = [f\"{f}_score\" for f in SCORE_COL_ORDER] + \\\n","                 [f\"{f}_score\" for f in FEATURES_ALL if f not in SCORE_COL_ORDER]\n","    ordered = [\"patient_id\",\"region\"] + img_cols + score_cols\n","    if has_source: ordered.append(source_col_name)\n","    out = out.reindex(columns=ordered)\n","\n","    if check_files:\n","        def _has_valid_img(row):\n","            for f in FEATURES_ALL:\n","                p = row[f\"{f}_img\"]\n","                if isinstance(p, str) and p:\n","                    return os.path.exists(p)\n","            return False\n","        mask = out.apply(_has_valid_img, axis=1)\n","        dropped = int((~mask).sum())\n","        out = out[mask].reset_index(drop=True)\n","        print(f\"[wide] Dropped {dropped} rows with non-existing image files.\")\n","\n","    out.to_csv(out_csv, index=False)\n","    print(f\"[wide] Wrote {out_csv} — rows={len(out)}, cols={len(out.columns)}\")\n","    return out\n","\n","\n","def preview_wide(path, n=5):\n","    df = pd.read_csv(path)\n","    print(f\"[preview] {path} — shape={df.shape}\")\n","    display(df.head(n))\n","    # per-feature row counts (which feature is filled)\n","    which = []\n","    for _, r in df.iterrows():\n","        fnd = None\n","        for f in FEATURES_ALL:\n","            if isinstance(r.get(f\"{f}_img\",\"\"), str) and r[f\"{f}_img\"]:\n","                fnd = f; break\n","        which.append(fnd)\n","    s = pd.Series(which, dtype=\"object\").value_counts().sort_index()\n","    print(\"[per-feature rows]\"); display(pd.DataFrame({\"rows\": s}))\n"],"metadata":{"id":"hQnS0tapPC7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CSV_SYN_LONG = \"/content/drive/MyDrive/Skin_project/dataset_generated_only.csv\"\n","CSV_SYN_WIDE = \"/content/drive/MyDrive/Skin_project/synthetic_as_wide.csv\"\n","\n","_ = long_to_wide_compatible(\n","        CSV_SYN_LONG, CSV_SYN_WIDE,\n","        check_files=True,     # set False if Drive isn't mounted yet\n","        keep_source=True      # keeps 'source' column (\"synthetic\")\n","    )\n","preview_wide(CSV_SYN_WIDE)\n"],"metadata":{"id":"QJA-Otc_PDie"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GDF Embeddigns Extraction code"],"metadata":{"id":"_ooYULQ2PHQx"}},{"cell_type":"code","source":["# ====== Paths / config ========================================================\n","CSV_TRAIN_POOL   = \"/content/drive/MyDrive/Skin_project/train_original.csv\"\n","#CSV_TRAIN_TRAD   = \"/content/drive/MyDrive/Skin_project/trad_aug_as_wide.csv\"\n","CSV_SYN_WIDE     = \"/content/drive/MyDrive/Skin_project/synthetic_as_wide.csv\"\n","CSV_TEST_FIXED   = \"/content/drive/MyDrive/Skin_project/test_original.csv\"\n","\n","# Keep redness out for now\n","FEATURES = [\"texture\",\"hyperpigmentation\",\"oiliness\",\"moisture\",\"elasticity\"]\n","\n","feature2imgcol = {\n","    \"moisture\": \"moisture_img\",\n","    \"oiliness\": \"oiliness_img\",\n","    \"elasticity\": \"elasticity_img\",\n","    \"texture\": \"texture_img\",\n","    \"redness\": \"redness_img\",\n","    \"hyperpigmentation\": \"hyperpigmentation_img\",\n","}\n","\n","# Base labels\n","DEFAULT_LABELS = [-1, 0, 1]\n","\n","# Binary remaps for the two very-imbalanced tasks\n","BINARY_MAP = {\n","    \"oiliness\": { -1: 0, 0: 1, 1: 1 },   # dry vs non-dry\n","    \"moisture\": { -1: 0, 0: 1, 1: 1 },   # low vs non-low\n","}\n","FEATURE_LABELS = {\n","    \"oiliness\":  [0, 1],\n","    \"moisture\":  [0, 1],\n","    # others keep DEFAULT_LABELS\n","}\n","\n","VAL_FRAC_WITHIN_TRAIN = 0.125\n","RANDOM_SEED = 42\n","\n","# Embeddings cache\n","CACHE_DIR = \"/content/drive/MyDrive/Skin_project/derm_emb_cache\"\n","os.makedirs(CACHE_DIR, exist_ok=True)\n","\n","BATCH_SIZE_INFER = 128\n","NUM_WORKERS      = min(8, os.cpu_count() or 4)"],"metadata":{"id":"NiuFpGr2EWoq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ====== Derm foundation model (inference signature only) =====================\n","derm_model = from_pretrained_keras(\"google/derm-foundation\")\n","derm_infer = derm_model.signatures[\"serving_default\"]"],"metadata":{"id":"e45kMo9-EdGM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Tune these if needed (smaller = less RAM)\n","ENCODE_WORKERS = 1            # threads that read & PNG-encode\n","ENCODE_CHUNK   = 64         # how many images to encode before inferring\n","STREAM_BATCH   = 32        # inference batch size for Derm (CPU)"],"metadata":{"id":"gak9r9wMZKml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _chunked(it, n):\n","    for i in range(0, len(it), n):\n","        yield it[i:i+n]\n"],"metadata":{"id":"rD7GX98zeEg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ====== Small progress helper =================================================\n","def make_phase_bar(desc, phases, position=1, leave=True):\n","    bar = tqdm(total=len(phases), desc=desc, position=position, leave=leave)\n","    idx = {\"i\": 0, \"phases\": phases}\n","    def tick(label=None):\n","        if label is None and idx[\"i\"] < len(idx[\"phases\"]):\n","            label = idx[\"phases\"][idx[\"i\"]]\n","        if label is not None: bar.set_postfix_str(str(label))\n","        bar.update(1); idx[\"i\"] += 1\n","    def close(): bar.close()\n","    return bar, tick, close\n","\n","# ====== Embedding I/O helpers =================================================\n","def _png_bytes_from_pil(pil_img: Image.Image) -> bytes:\n","    buf = io.BytesIO()\n","    pil_img.convert(\"RGB\").save(buf, format=\"PNG\")\n","    return buf.getvalue()\n","\n","def _example_from_png_bytes(image_bytes: bytes) -> bytes:\n","    ex = tf.train.Example(features=tf.train.Features(\n","        feature={'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))}\n","    ))\n","    return ex.SerializeToString()\n","\n","def _sha1(s: str) -> str:\n","    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n","\n","def _emb_cache_path(img_path: str) -> str:\n","    return os.path.join(CACHE_DIR, _sha1(img_path) + \".npy\")\n","\n","def _load_embed_if_cached(img_path: str):\n","    fp = _emb_cache_path(img_path)\n","    if os.path.exists(fp):\n","        try:\n","            arr = np.load(fp, allow_pickle=False)\n","            if arr.ndim == 1 and arr.shape[0] == 6144 and arr.dtype == np.float32:\n","                return arr\n","        except Exception:\n","            pass\n","    return None\n","\n","def _save_embed_to_cache(img_path: str, emb: np.ndarray):\n","    fp = _emb_cache_path(img_path)\n","    try: np.save(fp, emb.astype(np.float32), allow_pickle=False)\n","    except Exception: pass\n","\n","def _encode_one_path(img_path: str):\n","    try:\n","        with Image.open(img_path) as im:\n","            b = _png_bytes_from_pil(im)\n","        return img_path, _example_from_png_bytes(b)\n","    except Exception:\n","        return img_path, None\n","\n","def build_embeddings_cache(paths, desc=\"build cache\", position=2, allow_infer=True,\n","                           encode_workers=ENCODE_WORKERS, encode_chunk=ENCODE_CHUNK,\n","                           stream_batch=STREAM_BATCH, return_map=True):\n","    \"\"\"\n","    Streamed, RAM-safe cache builder.\n","    If return_map=False, embeddings are NOT accumulated in RAM (best for pre-cache).\n","    \"\"\"\n","    # Only unique, existing paths\n","    paths = [p for p in paths if isinstance(p, str) and os.path.exists(p)]\n","    uniq  = sorted(set(paths))\n","\n","    cache_map, missing = ({}, [])\n","    for p in uniq:\n","        emb = _load_embed_if_cached(p)\n","        if emb is not None:\n","            if return_map:\n","                cache_map[p] = emb\n","        else:\n","            missing.append(p)\n","\n","    if not missing or not allow_infer:\n","        if missing and not allow_infer:\n","            print(f\"⚠️ {len(missing)} missing embeddings; not inferring (allow_infer=False).\")\n","        return cache_map\n","\n","    # ---- encode + infer in bounded chunks ----\n","    pbar = tqdm(total=len(missing), desc=f\"{desc}\", unit=\"img\", position=position)\n","    for chunk_paths in _chunked(missing, encode_chunk):\n","        # Encode this chunk (bounded)\n","        encoded = []\n","        with ThreadPoolExecutor(max_workers=encode_workers) as ex:\n","            futs = {ex.submit(_encode_one_path, p): p for p in chunk_paths}\n","            for fut in as_completed(futs):\n","                p, ex_bytes = fut.result()\n","                if ex_bytes is not None:\n","                    encoded.append((p, ex_bytes))\n","\n","        # Infer this chunk in small batches and immediately write to disk\n","        for i in range(0, len(encoded), stream_batch):\n","            batch = encoded[i:i+stream_batch]\n","            if not batch:\n","                continue\n","            batch_paths = [p for p, _ in batch]\n","            batch_exs   = [exb for _, exb in batch]\n","            with tf.device(INFER_DEVICE):\n","                outs = derm_infer(inputs=tf.constant(batch_exs))\n","                embs = outs[\"embedding\"].numpy().astype(np.float32)\n","\n","            for p, e in zip(batch_paths, embs):\n","                # save to disk first\n","                _save_embed_to_cache(p, e)\n","                # keep in RAM only if requested\n","                if return_map:\n","                    cache_map[p] = e\n","\n","            # aggressively free batch buffers\n","            del batch, batch_paths, batch_exs, embs\n","            gc.collect()\n","\n","        # free the encoded chunk\n","        del encoded\n","        gc.collect()\n","\n","        pbar.update(len(chunk_paths))\n","    pbar.close()\n","    return cache_map\n","\n","\n","\n","def clear_embedding_cache():\n","    if not os.path.isdir(CACHE_DIR):\n","        print(\"No cache dir to clear.\"); return\n","    n = 0\n","    for fn in os.listdir(CACHE_DIR):\n","        if fn.endswith(\".npy\"):\n","            try:\n","                os.remove(os.path.join(CACHE_DIR, fn)); n += 1\n","            except Exception: pass\n","    print(f\"Cleared {n} cached files from {CACHE_DIR}.\")\n","\n","# ====== Data loading (handles binary remaps) =================================\n","def load_feature_df_one(csv_path, feature):\n","    df_all  = pd.read_csv(csv_path)\n","    img_col = feature2imgcol[feature]\n","    lbl_col = f\"{feature}_score\"\n","    cols    = [\"patient_id\", img_col, lbl_col] + ([\"region\"] if \"region\" in df_all.columns else [])\n","    df      = df_all[cols].copy()\n","    df      = df.rename(columns={img_col: \"image_path\", lbl_col: \"label\"})\n","\n","    # Keep rows that actually have an image on disk\n","    df = df[df[\"image_path\"].apply(lambda p: isinstance(p, str) and os.path.exists(p))]\n","    df = df.dropna(subset=[\"label\"])\n","    df[\"label\"] = df[\"label\"].astype(float).round().astype(int)\n","\n","    # Optional binary remap (oiliness/moisture)\n","    if feature in BINARY_MAP:\n","        df[\"label\"] = df[\"label\"].map(BINARY_MAP[feature])\n","        df = df.dropna(subset=[\"label\"]).astype({\"label\": int})\n","\n","    allowed = FEATURE_LABELS.get(feature, DEFAULT_LABELS)\n","    df = df[df[\"label\"].isin(allowed)].reset_index(drop=True)\n","    return df\n","\n","def load_feature_df_multi(csv_paths, feature):\n","    frames = [load_feature_df_one(p, feature) for p in csv_paths]\n","    df = pd.concat(frames, ignore_index=True)\n","    df = df.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n","    return df\n","\n","def df_to_Xy(df, emb_map):\n","    keep = [i for i, p in enumerate(df.image_path.values) if p in emb_map]\n","    if not keep:\n","        return np.zeros((0, 6144), np.float32), np.zeros((0,), np.int64), df.iloc[[]]\n","    df2 = df.iloc[keep].reset_index(drop=True)\n","    X   = np.stack([emb_map[p] for p in df2.image_path.values], axis=0)\n","    y   = df2.label.values.astype(np.int64)\n","    return X, y, df2\n","\n","def remove_patient_overlap(df_trainpool, df_test):\n","    overlap = set(df_trainpool.patient_id.unique()) & set(df_test.patient_id.unique())\n","    if overlap:\n","        print(f\" Removing {len(overlap)} overlapping patient_id(s) to prevent leakage.\")\n","        df_trainpool = df_trainpool[~df_trainpool.patient_id.isin(overlap)].reset_index(drop=True)\n","    return df_trainpool\n","\n","def make_val_from_trainpool(df_trainpool, val_frac_within_train=VAL_FRAC_WITHIN_TRAIN, seed=RANDOM_SEED):\n","    groups = df_trainpool[\"patient_id\"].values\n","    gss    = GroupShuffleSplit(n_splits=1, test_size=val_frac_within_train, random_state=seed)\n","    tr_idx, va_idx = next(gss.split(df_trainpool, groups=groups))\n","    return df_trainpool.iloc[tr_idx].reset_index(drop=True), df_trainpool.iloc[va_idx].reset_index(drop=True)\n","\n","# ====== Precache convenience ==================================================\n","#def precache_from_csvs(features, train_csvs, include_test=True, clear_cache=False):\n","    #if clear_cache: clear_embedding_cache()\n","    #paths = []\n","    #for feat in features:\n","        #for csvp in train_csvs:\n","           # df_trp = load_feature_df_one(csvp, feat); paths.extend(df_trp.image_path.values)\n","        #if include_test:\n","            #df_te = load_feature_df_one(CSV_TEST_FIXED, feat); paths.extend(df_te.image_path.values)\n","    #uniq = sorted({p for p in paths if isinstance(p, str) and os.path.exists(p)})\n","    #t0 = time.time()\n","    #cached_before = sum(os.path.exists(_emb_cache_path(p)) for p in uniq)\n","    #build_embeddings_cache(uniq, desc=\"precache\", allow_infer=True)\n","    #cached_after  = sum(os.path.exists(_emb_cache_path(p)) for p in uniq)\n","    #print(f\"Pre-cache: {len(uniq)} unique images | cached {cached_after} (new {cached_after-cached_before}) in {time.time()-t0:.1f}s.\")\n","\n","def precache_from_csvs(features, train_csvs, include_test=True, clear_cache=False):\n","    if clear_cache: clear_embedding_cache()\n","    paths = []\n","    for feat in features:\n","        for csvp in train_csvs:\n","            df_trp = load_feature_df_one(csvp, feat); paths.extend(df_trp.image_path.values)\n","        if include_test:\n","            df_te = load_feature_df_one(CSV_TEST_FIXED, feat); paths.extend(df_te.image_path.values)\n","\n","    uniq = sorted({p for p in paths if isinstance(p, str) and os.path.exists(p)})\n","\n","    t0 = time.time()\n","    cached_before = sum(os.path.exists(_emb_cache_path(p)) for p in uniq)\n","\n","    # STREAMED + DO NOT ACCUMULATE IN RAM\n","    build_embeddings_cache(\n","        uniq, desc=\"precache\", allow_infer=True,\n","        encode_workers=ENCODE_WORKERS, encode_chunk=ENCODE_CHUNK,\n","        stream_batch=STREAM_BATCH, return_map=False\n","    )\n","\n","    cached_after  = sum(os.path.exists(_emb_cache_path(p)) for p in uniq)\n","    print(f\"Pre-cache: {len(uniq)} unique images | cached {cached_after} (new {cached_after-cached_before}) in {time.time()-t0:.1f}s.\")\n"],"metadata":{"id":"AvGErO83Efot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9PXyX6BpeQcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1) Choosing which CSVs to use in the training pool\n","\n","\n","#TRAIN_CSVS = [CSV_TRAIN_POOL, CSV_TRAIN_TRAD]  # real + trad aug\n","TRAIN_CSVS = [CSV_TRAIN_POOL, CSV_SYN_WIDE]    # real + diffusion"],"metadata":{"id":"NDANOxxqM1Na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CACHE_DIR = \"/content/drive/MyDrive/Skin_project/derm_emb_cache\"\n","os.makedirs(CACHE_DIR, exist_ok=True)"],"metadata":{"id":"tjS_d7sMPyDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2) warm the cache once\n","precache_from_csvs(FEATURES, TRAIN_CSVS, include_test=True)"],"metadata":{"id":"jhMKX7qrM145"},"execution_count":null,"outputs":[]}]}